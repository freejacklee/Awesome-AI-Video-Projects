

## Awesome清单链接 ：[Awesome-AI-Video-Projects/List about the current collection of AI video projects.md at main · freejacklee/Awesome-AI-Video-Projects](https://github.com/freejacklee/Awesome-AI-Video-Projects/blob/main/List%20%20about%20the%20current%20collection%20of%20AI%20video%20projects.md)

* [Awesome-AI-Video-Projects/List about the current collection of AI video projects.md at main · freejacklee/Awesome-AI-Video-Projects] https://github.com/freejacklee/Awesome-AI-Video-Projects/blob/main/List%20%20about%20the%20current%20collection%20of%20AI%20video%20projects.md





## 仓库地址：freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects

* [freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects] https://github.com/freejacklee/Awesome-AI-Video-Projects





## 提交内容：List  about the current collection of AI video projects.md

This is an awesome GitHub list of information about the current collection of AI video projects

List  about the current collection of AI video projects





## 元资源

* [Wikipedia Text-to-video model](https://en.wikipedia.org/wiki/Text-to-video_model)

  * A **text-to-video model** is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning) model which takes as input a [natural language](https://en.wikipedia.org/wiki/Natural_language) description and produces a [video](https://en.wikipedia.org/wiki/Video) matching that description.[[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)
    文本到视频模型是一种机器学习模型，它将自然语言描述作为输入并生成与该描述匹配的视频。 [[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)

    Video prediction on making objects realistic in a stable background is performed by using [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) for a sequence to sequence model with a connector [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) encoding and decoding each frame pixel by pixel,[[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) creating video using [deep learning](https://en.wikipedia.org/wiki/Deep_learning).[[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)
    通过使用循环神经网络进行序列到序列模型的视频预测，使对象在稳定的背景下具有连接器卷积神经网络逐像素编码和解码每个帧， [[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) 使用深度学习创建视频。 [[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)

* [Wikipedia Generative artificial intelligence - Wikipedia](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)
  
  * Video 视频
    * Generative AI trained on annotated video can generate temporally-coherent video clips. Examples include Gen-1 and Gen-2 by [Runway](https://en.wikipedia.org/wiki/Runway_(company))[[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) and Make-A-Video by Meta Platforms.[[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)
      经过带注释视频训练的生成式人工智能可以生成时间连贯的视频剪辑。示例包括 Runway [[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) 的 Gen-1 和 Gen-2 以及 Meta Platforms 的 Make-A-Video。 [[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)



## AI generated video tool

### 1、Pika

Website: https://pika.art 
Discord: http://discord.gg/pika 
About: https://pika.art/about

官方社交媒体：

Twitter 推特 ： [Pika (@pika\_labs) / X](https://twitter.com/pika_labs)

小红书 官方号：Pika AIvideo (小红书号7027752781）

创始人之一 郭文景女士的推特账号 [Demi Guo (@demi\_guo\_) / X](https://twitter.com/demi_guo_?lang=en)

来自公众号 数字生命卡兹克的用户测评：*[【全网首发】PIKA1.0上手评测 - 你就是传奇](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660666&idx=1&sn=0e9e2a11d5c06cd512479d35ca84bf3f&sharer_shareinfo=d3d03bfa8311534acd00b7003123ed3c&sharer_shareinfo_first=d3d03bfa8311534acd00b7003123ed3c#rd)

我的个人体验：20231210已申请加入waitlist，暂未获得邀请资格，继续期待。我在Pika Labs的Discord上用一张图片生成的视频中人物的脸会变形。

温馨提示：

1、小红书app 搜 Jessie_Ma(小红书号946999884），可以先进Pika AI 聊天室2群，等待被邀请(12月18日15:00 Pika AI 聊天室2群邀请统计截止，据说很快会公测，官网申请加入waitlist即可)

2、目前可以在Pika Labs的Discord上直接用文字生成视频，指南如下：

* [Pika Labs在Discord上直接用文字生成视频  - YouTube](https://www.youtube.com/watch?v=d_GowRZE2cc)

* [Pika Labs Discord Guide --- Pika Labs Discord 指南](https://pikalabs.org/pika-labs-discord-guide/)



趣闻：

* [哈佛斯坦福学霸女儿创业AI项目走红全球，老爸公司两个涨停：仅是父女，没投钱](https://baijiahao.baidu.com/s?id=1784049635636388017&wfr=spider&for=pc)

  * A股又现“女儿概念股”。

    近日，人工智能应用Pika走红，该应用创始人的父亲实控的A股上市公司信雅达也被带火，12月1日开盘继续一字涨停，拿下两连板。

    11月30日，信雅达科技股份有限公司（信雅达，600571）发布澄清公告称，近日，信雅达关注到媒体关于“视频生成应用Pika”的相关报道，为避免相关信息对广大投资者造成误导，现予以澄清说明。

    信雅达表示，Pika开发团队创始人之一郭文景系公司实际控制人郭华强的女儿。除上述关系外，公司与Pika无其他关系。

    公告强调，截至目前，郭文景未在公司担任任何职务，公司未投资Pika，也未与Pika有任何业务往来。

    此事源于人工智能产品Pika1.0在网络爆火。




媒体资讯：

* [我用了2周PIKA1.0后，总结了10个宝藏使用技巧 - 建议收藏公测后用](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660789&idx=1&sn=756a1ca3f9c3fb7b9edf9f248ee9863e&sharer_shareinfo=a5e572e39b7ac90935b67162d22c0305&sharer_shareinfo_first=a5e572e39b7ac90935b67162d22c0305#rd)
  * 之前的PIKA1.0首发评测，可以看我的这篇文章：【全网首发】PIKA1.0上手评测 - 你就是传奇
    关于PIKA的使用技巧，这次我自己总结了10个，个人感觉还挺有用的，在PIKA即将全面公测之际，分享给大家，希望对大家后面使用PIKA1.0，会有一些帮助。

* [Pika爆火，但AI视频还没到「GPT时刻」-36氪](https://36kr.com/p/2558743578239112)

  * 某种程度上，这其实反映出了一个趋势：**比起文生图的竞争，在更高门槛的AI视频，创业公司寻求商业化的意愿更强烈。**

    产生上述焦虑的原因也并不难理解。 

    **一是算力的掣肘，视频领域对算力需求更高。** Pika联创就曾举过一个例子：“对于 Stable Diffusion，有人可能用8张A100就能从头开始学习，并得到不错的结果。但对于视频模型，用8张A100可能不够了，可能无法训练出一个好的模型。” 

    她甚至坦言，开源社区可能没有足够的算力来训练新的视频模型，除了一些大公司开源模型外，普通开源社区很难进行探索性工作。 

    **二是竞争环境的激烈。** 在AI视频产品层面，一方面正如上文所梳理的，头部科技巨头基本都已入局，只是产品尚未全面公测。另一方面，也包括了如Adobe此类面向专业级用户的老牌软件巨头和如已有先发优势的Runway。 

    还有一类则是HeyGen、Descript、CapCut类的轻量化视频制作产品。 

    大型科技公司具备算力优势，特别在是目前尚未有巨头明确开源路线（只有Stability AI发布了开源生成式视频模型Stable Video Diffusion）。而Adobe此类企业的优势在于AI视频功能和原有业务形成有力的协同，形成更高频的使用。Adobe此前也收购了一家AI视频领域的初创公司Rephrase.ai。 

    而轻量化的视频制作产品本身面向的是非专业人群，这意味着能否以差异化优势快速圈中人群，占据心智成为关键。 

    套用一句老生常谈，人们对技术的态度永远是高估短期，低估长期，AI视频也并不例外。 


* [Pika on X: "We’re beginning to let people in off the waitlist, and want to welcome our first Pika 1.0 users! Here’s how to start creating with text-to-video. Sign up at https://t.co/nqzjGy82Lx https://t.co/GXVx9WhM2i" / X](https://twitter.com/pika_labs/status/1734311655771673040)

  * 我们开始让人们进入候补名单，并希望欢迎我们的第一个Pika 1.0用户！

    这是开始使用文本到视频创建的方法。

    请注册https://pika.art/waitlist

* [AI生成视频工具Pika爆火，估值超2亿美元-虎嗅网](https://m.huxiu.com/article/2361484.html#:~:text=%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%8D%8E%E4%BA%BA%E5%AD%A6%E7%94%9F%E9%80%80%E5%AD%A6%E5%88%9B%E5%8A%9E,%E7%BC%96%E8%BE%91%E5%92%8C%E9%87%8D%E6%96%B0%E6%9E%84%E6%83%B3%E5%9C%BA%E6%99%AF%E3%80%82&text=%F0%9F%92%A5%20Pika%201.0%E4%BD%BF%E7%94%A8AI,%E7%AE%80%E5%8D%95%E4%B8%94%E9%A3%8E%E6%A0%BC%E5%A4%9A%E5%8F%98%E3%80%82)
  * 斯坦福华人学生退学创办的AI视频生成工具Pika 1.0正式推出，估值超过2亿美元。 该工具可以通过文字、图片和视频生成高质量的各种风格视频，并且支持用户上传视频片段进行编辑和重新构想场景。 💥 Pika 1.0使用AI模型生成非常贴近生动的视频，使用简单且风格多变。

* [斯坦福华人博士文生视频Pika 1.0爆火，4人公司估值2亿，OpenAI联创参投-36氪](https://36kr.com/p/2539021165094660) 

  * [新智元](https://36kr.com/user/574825230) 发表于2023-11-29 15:30

  * Runway Gen-2最强竞品Pika，暌违半年忽然放出大招——Pika 1.0正式发布！

    仅成立六个月，Pika就结束了测试版，正式发布了第一个产品，能够生成和编辑3D动画、动漫、卡通和电影。


* [斯坦福华人博士文生视频Pika 1.0爆火！4人公司估值2亿，OpenAI联创参投](https://mp.weixin.qq.com/s?__biz=MzU0OTkwNTM2Mw==&mid=2247653802&idx=1&sn=d8597cc271043b01541ff96aa561fc22&sharer_shareinfo=2d57f4d178c2f63c3692057c22223146&sharer_shareinfo_first=2d57f4d178c2f63c3692057c22223146#rd)
  * 在获取Pika 1.0试用资格之前，和Midjourney一样，用户现在通过Discord获取Pika Labs的视频生成服务。用户只需在聊天框输入文字，比如「一个机器人在日落沙滩上行走」，就能收到一个由AI生成的视频。
    周二，Pika把这一体验带到了网页上，面向更广泛的主流群体，让他们可以在编辑视频、自定义物体。这里还有一段，Pika创意总监前几天放出的，用Pika文本转视频AI功能制作的「3D动画预告片」，效果萌到爆。
* [4个人，Pika估值10亿](https://mp.weixin.qq.com/s?__biz=Mzk0ODUwNjUxNQ==&mid=2247484633&idx=1&sn=bf28878224f5ef7205214401a7d5669a&sharer_shareinfo=9978076d64328890c7eb7018395ec70d&sharer_shareinfo_first=9978076d64328890c7eb7018395ec70d#rd)
  * 要了解关于Pika的最新信息，请关注X上的@pika_labs，加入pika的Discord社区并在这里访问我们的测试版产品，并在https://pika.art加入新Pika 1.0的等待名单。
* [中国天才少女硅谷创立AI公司，半年估值超10亿](https://mp.weixin.qq.com/s?__biz=MTI3NTQ1MTY0MQ==&mid=2650603935&idx=1&sn=368bd29cbf96b8095e5541ae6689b229&sharer_shareinfo=eda2394bbef20b16a62fb59bd91e59ac&sharer_shareinfo_first=eda2394bbef20b16a62fb59bd91e59ac#rd)
  * 目前，Pika1.0正式的网页版需要排队预约，尚未有用户实际测评过。有人借此质疑，横空出世的Pika一夜爆红，是否为一场营销骗局？毕竟，在11月之前，Pika还只是一个无名之辈。事实上，Pika的第一个版本今年4月下旬就在Discord上进行了公测。7月，在Discord正式推出服务器，并在几个月时间内收获了50万用户。由于Pika团队精简，寄生在Discord平台，能够最大限度地减少开发量。
    最初，Pika只支持文本生视频，后来逐渐支持图片转视频、相机控制、文字和Logo嵌入视频中等。Pika1.0宣传片中的许多功能，目前Discord上的版本并不支持，只能等网页版开放测评后验证。Pika也并非第一次在众人前亮相。今年11月初，《流浪地球3》的发布会上，电影工业化实验室G!Lab官宣成立。郭帆导演介绍了一批战略合作的科技公司，包括商汤科技、小米、华为等，还有Pika Labs。至今，成立仅6个月的Pika已经完成了三轮融资，总金额5500万美元，估值超10亿元人民币。投资人阵容也可谓豪华——包括OpenAI董事会成员Adam D'Angelo与前特斯拉AI总监Andrej Karpathy、前Github CEO Nat Friedman、YC合伙人Daniel Gross，以及硅谷著名投资人Elad Gil等。
* [Pika, which is building AI tools to generate and edit videos, raises $55M | TechCrunch --- Pika 正在构建用于生成和编辑视频的 AI 工具，筹集了 5500 万美元 | TechCrunch](https://techcrunch.com/2023/11/28/pika-labs-which-is-building-ai-tools-to-generate-and-edit-videos-raises-55m/)




@阑夕
AI视频生产，新兴产品Pika和老牌产品Runway
的对比，同一张图片、同样的设置，可以看得出来已经各有千秋了。

来自阑夕的微博视频号 https://weibo.com/tv/show/1034:4977370101383191?from=old_pc_videoshow





### 2、Runway Gen-2

Website: https://research.runwayml.com/gen2

Discord: https://discord.com/invite/runwayml

About: https://research.runwayml.com/about

Twitter 推特 官方社交媒体：[Runway (@runwayml) / X](https://twitter.com/runwayml)

Date：2023年6月8日

* [Runway on X: "Gen-2: Text to Video is here. Learn the basics with today's Runway Academy. Watch the video: https://t.co/E5HEKrJpNX" / X](https://twitter.com/runwayml/status/1666793633524195328)



Summary：

Runway's Gen-2 allows users to create videos in any style using Text to Video generation. Runway 的 Gen-2 

允许用户使用文本到视频生成功能创建任何风格的视频。



Tags：

AI, video generation, Runway

人工智能、视频生成、Runway



Gen-2 Explained：

Not too long ago, runway pushed the boundaries of generative Ai with Gen One a video to video model that allows you to use words and images to generate new videos out of existing ones in the week since launching, the model has constantly gotten better temporal consistency, better fidelity better results and as more and more people gained access, we unlocked entirely new use cases and displays of creativity and today we're excited to announce our biggest unlock yettext to Video with Gen Two now. You can generate a video with nothing but words, no driving video no input image gen 2 represents yet another major research milestone and another monumental step forward for generative Ai with Gen 2, anyone anywhere can suddenly realize entire worlds, animations stories anything you can imagine gen two coming, very soon to https://runwayml.com/

不久前，runway通过Gen One 突破了生成式AI的界限，这是一个视频到视频模型，允许您使用文字和图像从现有视频中生成新视频，自推出以来的一周内，该模型不断获得更好的时间一致性，更好的保真度，更好的结果，并且随着越来越多的人获得访问权限， 我们解锁了全新的用例和创造力展示，今天我们很高兴地宣布，我们迄今为止最大的解锁版本是第二代视频。你可以生成一个只有文字的视频，没有驾驶视频，没有输入图像，第二代代表了另一个重要的研究里程碑，也是生成式人工智能向前迈出的又一重大一步，第二代，任何地方的任何人都可以突然意识到整个世界，动画故事、任何你能想象到的第二代即将到来，很快就会 https://runwayml.com/

个人体验：20231212尝试了一次，用一张弹古筝的女子图片和一句简短的文字生成视频，生成的视频中随着时间流逝人物面部会有点变形。

* [What is Gen-2 AI and How to Use It? A Step-by-Step Guide --- 什么是第二代人工智能以及如何使用它？分步指南](https://ambcrypto.com/blog/what-is-gen-2-and-how-to-use-it-a-step-by-step-guide/)

  * Gen-2 AI is the second generation of Runway’s AI software, which focuses on generating videos from scratch using text descriptions, images, or existing video clips. 
    Gen-2 AI 是 Runway 的第二代 AI 软件，专注于使用文本描述、图像或现有视频剪辑从头开始生成视频。

    This cutting-edge technology opens a realm of possibilities for content creators, allowing them to craft distinctive and captivating videos without resorting to costly equipment or lengthy procedures.
    这项尖端技术为内容创作者打开了一个可能性的领域，使他们能够制作独特且引人入胜的视频，而无需诉诸昂贵的设备或冗长的程序。

    The Gen-2 AI model is designed to help users create dreamy videos by harnessing the power of AI and generative algorithms, allowing for an unparalleled level of customization and fidelity in the final output. 
    Gen-2 AI 模型旨在帮助用户利用 AI 和生成算法的力量来创建梦幻视频，从而在最终输出中实现无与伦比的定制化和保真度。

    With Gen-2 AI, the days of being limited by available footage or budget constraints are numbered, as this innovative technology brings endless creative possibilities to the table.
    有了第二代人工智能，受可用镜头或预算限制的日子已经屈指可数了，因为这项创新技术带来了无限的创意可能性。

* [全面开放，无需排队，Runway视频生成工具Gen-2开启免费试用](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650885185&idx=3&sn=d8042976cd29c23e0c0ba8ee1923a645)
  * Runway 宣布，Gen-1 和 Gen-2 已经彻底开放，任何人都可以注册一个账号免费尝试。生成的视频长度为 4 秒，每秒消耗 5 个积分，利用免费额度可以生成二十几个视频。如果免费积分耗尽，付费标准为 0.01 美元 / 积分，也就是生成一个视频需要 0.2 美元。*2023-07-25 13:24* *机器之心 发表于北京*

* [图像涂哪就动哪，Gen-2新功能“神笔马良”爆火，网友：急急急-36氪](https://36kr.com/p/2515454214115330)

  

#### Runway Gen-1

Introducing Gen-1: A New AI Model for Video Generation

Website: https://research.runwayml.com/gen1

About: https://research.runwayml.com/about

Paper: 2023 [Structure and Content-Guided Video Synthesis with Diffusion Models](https://arxiv.org/abs/2302.03011)

Date：2023年2月7日

* [Runway on X: "Today, Generative AI takes its next big step forward. Introducing Gen-1: a new AI model that uses language and images to generate new videos out of existing ones. Sign up for early research access: https://t.co/7JD5oHrowP https://t.co/4Pv0Sk4exy" / X](https://twitter.com/runwayml/status/1622594989384519682)



Summary：

Runway has announced the launch of Gen-1, a new AI model that uses language and images to generate new videos out of existing ones. Early research access is available by signing up on their website. 

Runway 宣布推出 Gen-1，这是一种新的人工智能模型，可以使用语言和图像从现有视频中生成新视频。通过在其网站上注册即可获得早期研究访问权限。



Tags：

AI, video generation, Generative AI, Runway

人工智能、视频生成、生成式人工智能、Runway



Gen-1 Explained：

Gen 1 is able to realistically and consistently apply the composition and style of an image or text prompt to the target video allowing you to generate new video content using an existing video.We call this approach video to video, and we're incredibly excited to share a few early use casesstylization mode, transfer the style of any image or prompt to every frame of your video storyboard mode, turn mockups into fully stylized and animated rendersmask mode, isolate subjects in your video and modify them with simple text prompts.Render mode, turn untextured renders into realistic outputs by applying an input imageor prompt.
To realizing the future of storytelling.

Gen 1 能够逼真且一致地将图像或文本提示的构图和样式应用于目标视频，从而允许您使用现有视频生成新的视频内容。我们将这种方法称为视频到视频，我们非常高兴地分享一些早期的用例风格化模式，将任何图像或提示的样式传输到视频故事板模式的每一帧，将模型转换为完全风格化和动画渲染蒙版模式，隔离视频中的主题并使用简单的文本提示对其进行修改。渲染模式，通过应用输入图像或提示将无纹理渲染转换为逼真的输出。
实现讲故事的未来。





### 3、WonderJourney

项目及演示：https://kovenyu.com/wonderjourney/
论文：2023 [WonderJourney: Going from Anywhere to Everywhere](https://arxiv.org/abs/2312.03884)
GitHub：https://github.com/KovenYu/WonderJourney（Coming soon!）



[ [小互 on X: "WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。 它能够根据用户提供的文本描述或图片，自动生成一系列3D场景的连续画面。 这些场景不仅多样化，而且彼此之间还能紧密衔接，形成一种虚拟的“奇妙旅程”场景。 而且你只需要输入一段描述或上传一张图片即可... 主要功能特点：… https://t.co/gptrWSyWBz" / X](https://twitter.com/xiaohuggg/status/1733779657722622449)](https://twitter.com/xiaohuggg/status/1733779657722622449)

* 发表于2023-12-10 17:24 

* WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。

  它能够根据用户提供的文本描述或图片，**自动生成一系列3D场景的连续画面。**

  这些场景不仅多样化，而且**彼此之间还能紧密衔接**，形成一种**虚拟的“奇妙旅程”场景**。

  **而且你只需要输入一段描述或上传一张图片即可...**

  

  **主要功能特点：**

  与之前专注于单一场景类型的视图生成工作不同，WonderJourney从任何用户提供的位置（通过文本描述或图像）开始，生成一系列多样化但连贯相连的3D场景。

  更具体的主要功能特点详见上述推特

  

  **工作原理：**

  该框架利用大语言模型（LLM）生成场景的文本描述，一个由文本驱动的点云生成管道来制作引人入胜且连贯的3D场景序列，以及一个视觉语言模型（VLM）来验证生成的场景。

  更具体的工作原理详见上述推特



[WonderJourney谷歌合作的 3D 场景生成, 带你走进“奇妙旅程”](https://mp.weixin.qq.com/s?search_click_id=12501261693348582618-1702361382506-3224040202&__biz=Mzg2ODk4MDUxOQ==&mid=2247485556&idx=1&sn=b71b69562cab374580961cd61e05b976#rd)

* 给定诗歌或故事提要等一系列文本描述,WonderJourney也可以生成古诗词场景。





### 4、Text-to-Video Tool（Create mini AI videos from text）

URL： [TextToVideo | Create videos from text](https://text-to-video.vercel.app/)

Newsletter Post Title：ChatGPT business ideas with a billionaire

Newsletter Post URL：https://bensbites.beehiiv.com/p/government-bans-will-ai-emerge-victorious

Date：2023年4月3日



Summary：

This link leads to a website that offers a text-to-video tool. Users can input text and the tool will generate a video based on the content. The website also offers customization options for the video's appearance and background music.

此链接指向一个提供文本转视频工具的网站。用户可以输入文本，该工具将根据内容生成视频。该网站还提供视频外观和背景音乐的自定义选项。



Tags：

text-to-video, video generation, AI tool

文本转视频、视频生成、AI工具



来自 Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

* [Text to Video AI - Product Information, Latest Updates, and Reviews 2023 | Product Hunt --- 文本转视频 AI - 2023 年产品信息、最新更新和评论 |产品搜索](https://www.producthunt.com/products/text-to-video-ai)

  * Text to Video AI 文本转视频人工智能

  * Launched on March 31st, 2023

    Text to Video is my latest project, which allows you to create videos using AI. Currently AI videos are in their "monstrous stage", just like Dalle 2 MINI a while back. The project seeks that people can have a first approach to text-to-video.
    文本到视频是我的最新项目，它允许您使用人工智能创建视频。目前人工智能视频正处于“怪物阶段”，就像不久前的 Dalle 2 MINI 一样。该项目旨在让人们能够拥有第一种将文本转为视频的方法。





* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * **阿里的Animate Anyone**

    **字节跳动的MagicAnimate**

    **微软的GAIA**

* [阿里、字节悄悄上线AI神器，让梅西跳舞不在话下-36氪](https://36kr.com/p/2549019770755460)

* [阿里大战字节！Animate Anyone vs Magic Animate！AI短视频领域的学术之争！背后技术谁更强？ - YouTube](https://www.youtube.com/watch?v=L-iXbg-GTXk)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下 | 论文频道 | 领研网](https://www.linkresearcher.com/theses/6e61b037-0fbd-4e39-a406-baef6e27afed)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下-36氪](https://36kr.com/p/2543100034213636)

### 5、阿里的Animate Anyone

阿里推出了Animate Anyone，该项目由阿里巴巴智能计算研究院开发，你只需提供一个静态的角色图像（包括真人、动漫/卡通角色等）和一些动作、姿势（比如跳舞、走路），便可将其动画化，同时保留角色的细节特征（如面部表情、服装细节等）。

阿里项目：https://github.com/HumanAIGC/AnimateAnyone

**项目（Animate Anyone官网）地址：**https://humanaigc.github.io/animate-anyone/

阿里论文：2023  [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117)
阿里相关论文发布于2023年11月28日

* [有媒体报道，最近很火... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NwMlp5iDZ?type=repost)

  * 宝玉xp

    23-12-12 13:55
    有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的
    
    转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练 

    微博文字部分略，详见上面的网页链接

    ——来源：* [Alibaba's 'Animate Anyone' Is Trained on Scraped Videos of Famous TikTokers](https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

* [阿里巴巴智能计算研究院打造Animate Anyone：一种革命性的角色动画技术 | ATYUN.COM 官网-人工智能教程资讯全方位服务平台](https://www.atyun.com/57976.html)

  * 文字部分略，详见上面的网页链接

    文章来源：https://analyticsindiamag.com/this-new-ai-tool-could-mark-the-beginning-of-the-end-for-tiktok-and-instagram-influencers/

Animate Anyone在Huggingface上的在线测试地址：暂未发现已经开源 * [AnimateAnyone (AnimateAnyone)](https://huggingface.co/AnimateAnyone)



### 6、字节跳动的MagicAnimate

Magic Animate是一项开创性的开源项目，简化了动画创作，允许您从单个图像和动态[视频](https://www.yjpoo.com/shipinsucai/)制作动画视频，简单来说，给定一张参考图像和一个姿态序列（视频），它可以生成一个跟随姿态运动，并保持参考图像身份特征的动画视频。由新加坡国立大学的Show Lab和字节跳动打造。

Magic Animate在所有舞蹈视频解决方案中提供最高的一致性，但是Magic Animate 面部和手部可能会出现一些扭曲。默认配置可能会导致从动漫到写实主义的风格转变，尤其是在视频中的面部。将动漫风格应用于默认的DensePose驱动视频也会影响身体比例。

**Magic Animate官网地址：**www.magicanimate.org

字节项目：https://github.com/magic-research/magic-animate

字节论文：2023  [MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://arxiv.org/abs/2311.16498)

字节相关论文发布于2023年11月27日

MagicAnimate在Huggingface上的在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate

* [抖音跳舞不用真人出镜，一张照片就能生成高质量视频-虎嗅网](https://m.huxiu.com/article/2389861.html?f=rss)

  * 这就是来自新加坡国立大学和字节跳动最新的一项研究，名叫**MagicAnimate**。

    它的作用简单来说可以总结为一个公式：一张**图片** + 一组**动作** = 毫无违和感的**视频**。

* [新加坡国立大学和字节... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NvETcaeD1)

  * 宝玉xp

    23-12-5 05:06
    来自 微博视频号
    新加坡国立大学和字节跳动联合推出的MagicAnimate，可以用一张照片加上骨骼动画制作小姐姐跳舞视频。这和前几天阿里推出的 AnimateAnyone 网页链接 很像。
    
    MagicAnimate：通过扩散模型创造时间连贯的人像动画，并提供了 Gradio 演示

    本地演示链接:* [magic-research/magic-animate: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://github.com/magic-research/magic-animate#-gradio-demo?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

    本论文深入探讨了人像动画任务——其核心目标是制作一段视频，展示一个特定人物身份按照既定动作序列进行动作。中间文字略，详见上面的网页链接，经验证实，我们的方法在两个基准测试中均优于现有的基准方法。特别是在挑战性极高的 TikTok 舞蹈数据集上，我们的方法在视频真实度方面比最强基线提高了超过 38%。我们将公开代码和模型。

    论文：https://arxiv.org/abs/2311.16498
    项目首页：https://showlab.github.io/magicanimate/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38
    代码：https://github.com/magic-research/magic-animate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38 宝玉xp的微博视频 https://weibo.com/tv/show/1034:4975452565995522?from=old_pc_videoshow
    在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38
    
    #微博新知#

阿里、字节两公司在Github上的开源文件还在不断更新中。



### 7、微软的GAIA

项目地址：https://microsoft.github.io/GAIA/       12月14日访问该网页显示404错误

Paper：2023 [GAIA: Zero-shot Talking Avatar Generation](https://arxiv.org/abs/2311.15230)

GitHub：https://github.com/microsoft/GAIA  12月14日访问该网页显示404错误

* [How to Create Talking Virtual Characters with Microsoft GAIA - YouTube](https://www.youtube.com/watch?v=mwsfS0dq_bc)

* [Stunning Breakthroughs in AI Creativity! - YouTube](https://www.youtube.com/watch?v=yfxZKoTOka0)

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制 | 机器之心](https://www.jiqizhixin.com/articles/2023-12-04-4)

  * 视频 PS 可以灵活到什么程度？最近，微软的一项研究提供了答案。

    在这项研究中，你只要给 AI 一张照片，它就能生成照片中人物的视频，而且人物的表情、动作都是可以通过文字进行控制的。比如，如果你给的指令是「张嘴」，视频中的人物就会真的张开嘴。

    。。。。。。

    这项研究名叫 GAIA（Generative AI for Avatar，用于虚拟形象的生成式 AI），其 demo 已经开始在社交媒体传播。不少人对其效果表示赞叹，并希望用它来「复活」逝者。

    但也有人担心，这些技术的持续进化会让网络视频变得更加真假难辨，或者被不法分子用于诈骗。看来，反诈手段要继续升级了。

    **GAIA 有什么创新点？**

    会说话的虚拟人物生成旨在根据语音合成自然视频，生成的嘴型、表情和头部姿势应与语音内容一致。以往的研究通过实施特定虚拟人物训练（即为每个虚拟人物训练或调整特定模型），或在推理过程中利用模板视频实现了高质量的结果。最近，人们致力于设计和改进零样本会说话的虚拟人物的生成方法（即仅有一张目标虚拟人物的肖像图片可以用于外貌参考）。不过，这些方法通过采用基于 warping 的运动表示、3D Morphable Model（3DMM）等领域先验来降低任务难度。这些启发式方法虽然有效，但却阻碍了从数据分布中直接学习，并可能导致不自然的结果和有限的多样性。

    本文中，来自微软的研究者提出了 GAIA（Generative AI for Avatar），其能够从语音和单张肖像图片合成自然的会说话的虚拟人物视频，在生成过程中消除了领域先验。

    

    GAIA 揭示了两个关键洞见：

    1. 用语音来驱动虚拟人物运动，而虚拟人物的背景和外貌（appearance）在整个视频中保持不变。受此启发，本文将每一帧的运动和外貌分开，其中外貌在帧之间共享，而运动对每一帧都是唯一的。为了根据语音预测运动，本文将运动序列编码为运动潜在序列，并使用以输入语音为条件的扩散模型来预测潜在序列；
    2. 当一个人在说出给定的内容时，表情和头部姿态存在巨大的多样性，这需要一个大规模和多样化的数据集。因此，该研究收集了一个高质量的能说话的虚拟人物数据集，该数据集由 16K 个不同年龄、性别、皮肤类型和说话风格的独特说话者组成，使生成结果自然且多样化。

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制-36氪](https://36kr.com/p/2543099800561414)

* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * 文字部分略，详见上面的网页链接

* [微软的这个项目厉害了... - @互联网的那点事的微博 - 微博](https://weibo.com/1627825392/Nv8Ya23gw?type=repost)

  * 互联网的那点事

    23-12-1 19:51
    发布于 安徽
    来自 微博视频号
    文字部分略，详见上面的网页链接 https://weibo.com/tv/show/1034:4974225992384581?from=old_pc_videoshow

* [Xu Tan on X: "🔥GAIA (Generative AI for Avatar) generates high-quality, natural, and spontaneous avatars given a single reference image driven by text/speech/video. ‼️ Some keywords of GAIA: Data/Model Scaling, ID/Motion Disentanglement, and Zero-Shot. 🔗Project page: https://t.co/qQUXlaAPmU" / X](https://twitter.com/xutan_tx/status/1731007471484027036)
  * GAIA (Generative AI for Avatar)（头像生成式人工智能）通过文本/语音/视频驱动，在单一参考图像的基础上生成高质量、自然、自发的头像。
    GAIA 的一些关键词 数据/模型缩放、ID/运动离散和零镜头。

* [Dreaming Tulpa 🥓👑 on X: "High quality AI generated talking heads are coming! GAIA can generate talking avatars from a single portrait image and speech clip. It even supports text prompts like \`sad\`, \`open mouth\` or \`surprise\` to guide video generation. Crazy times ahead 🤯 https://t.co/20WZOLMypz https://t.co/kgYLyzE1RJ" / X](https://twitter.com/dreamingtulpa/status/1730514359317590234)

  * 高品质人工智能生成的头像来了！

    GAIA 可以从单个肖像图像和语音片段生成会说话的化身。它甚至支持“悲伤”、“张开嘴”或“惊讶”等文字提示来指导视频生成。疯狂的时代即将来临



### 8、GitHub - arpitbansal297/Universal-Guided-Diffusion

Paper（论文地址）: 2023 [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121v1)

GitHub（代码地址）：https://github.com/arpitbansal297/Universal-Guided-Diffusion

Newsletter Post Title：AI in the workplace

Newsletter Post URL：

* [AI in the workplace](https://bensbites.beehiiv.com/p/ai-workplace)
  * Multimodal universal guidance for diffusion models without retraining.

Date：2023年6月8日



Summary：

This is a GitHub repository for Universal Guided Diffusion, which is a machine learning model for image and video generation. 

这是通用引导扩散的 GitHub 存储库，通用引导扩散是一种用于图像和视频生成的机器学习模型。



Tags：

machine learning, image generation, video generation, GitHub

机器学习、图像生成、视频生成、GitHub



* [Universal-Guided-Diffusion/README.md at main · arpitbansal297/Universal-Guided-Diffusion](https://github.com/arpitbansal297/Universal-Guided-Diffusion/blob/main/README.md)
  * The official PyTorch implementation of [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121). This repository has python implementation of universal guidance algorithm that enables controlling diffusion models by arbitrary guidance modalities without the need to retrain any use-specific components. Different guidance modalities we demonstrate are Human Identity, Segmentation Maps, Object Location, Image Style and Clip. Our implementation is based on the text-to-img model from [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and Imagenet Diffusion Model from [OpenAI's guided diffusion](https://github.com/openai/guided-diffusion).
    扩散模型通用指南的官方 PyTorch 实现。该存储库具有通用制导算法的 python 实现，可以通过任意制导方式控制扩散模型，而无需重新训练任何特定用途的组件。我们展示的不同引导模式包括人类身份、分割图、对象位置、图像样式和剪辑。我们的实现基于来自稳定扩散的文本到图像模型和来自 OpenAI 引导扩散的 Imagenet 扩散模型。

* [Universal Guidance for Diffusion Models,arXiv - CS - Machine Learning - X-MOL](https://newsletter.x-mol.com/paper/1626022089827893248?adv)

  * **扩散模型的通用指南**

    典型的扩散模型经过训练可以接受特定形式的调节，最常见的是文本，并且不能在没有重新训练的情况下以其他形式为条件。在这项工作中，我们提出了一种通用的指导算法，使扩散模型能够由任意指导方式控制，而无需重新训练任何特定于使用的组件。我们展示了我们的算法成功地生成了具有引导功能的高质量图像，包括分割、人脸识别、对象检测和分类器信号。代码可在 https://github.com/arpitbansal297/Universal-Guided-Diffusion 获得。

* [扩散模型的通用指导手册\_Zilliz\_InfoQ写作社区](https://xie.infoq.cn/article/c3c7dfb1947a98fddb858a36d)

  * 文字部分略，详见上面的网页链接

    

    

### 9、SceneScape: Text-Driven Consistent Scene Generation

项目地址：https://scenescape.github.io/

Paper（论文地址）: 2023 [SceneScape: Text-Driven Consistent Scene Generation](https://arxiv.org/abs/2302.01133)

Supplementary Material：https://scenescape.github.io/sm/index.html

GitHub（代码地址）：https://github.com/RafailFridman/SceneScape

Date：2023年2月3日



Summary：

The paper presents a method for text-driven perpetual view generation, which synthesizes long-term videos of various scenes solely based on an input text prompt. The method combines the generative power of a pre-trained text-to-image model with the geometric priors learned by a pre-trained monocular depth prediction model to achieve 3D consistency. The depth maps are used to construct a unified mesh representation of the scene, which is progressively constructed along the video generation process. The method generates diverse scenes, such as walkthroughs in spaceships, caves, or ice castles. 

本文提出了一种文本驱动的永久视图生成方法，该方法仅根据输入的文本提示合成各种场景的长期视频。该方法将预训练的文本到图像模型的生成能力与预训练的单目深度预测模型学习的几何先验相结合，以实现 3D 一致性。深度图用于构建场景的统一网格表示，该表示是沿着视频生成过程逐步构建的。该方法生成不同的场景，例如宇宙飞船、洞穴或冰堡中的演练。

How It Works ：

> ```
> We represent the generated scene with a unified mesh , which is constructed in an online fashion. Given a camera at Ct+1, at each synthesis step, a new frame is generated by projecting t into Ct+1, and synthesizing the newly revealed content by using a pre-trained text-to-image diffusion model. To estimate the geometry of the new synthesized content, we leverage a pre-trained depth prediction model; to ensure the predicted depth is consistent with the existing scene t, we deploy a test-time training, encouraging the predicted depth by the model to match the projected depth from t. We then update our mesh representation to form t+1 which includes the new scene content.
>  
> 我们用统一的网格  表示生成的场景，该网格以在线方式构建。给定 Ct+1 处的相机，在每个合成步骤中，通过将 t 投影到 Ct+1 中生成一个新帧，并使用 pre 合成新显示的内容-训练有素的文本到图像扩散模型。为了估计新合成内容的几何形状，我们利用预先训练的深度预测模型；为了确保预测深度与现有场景 t 一致，我们部署了测试时训练，鼓励模型的预测深度与 t 的预测深度相匹配。然后，我们更新网格表示以形成 t+1 ，其中包括新的场景内容。
>  
>  
>  A visualization of the resulting meshes, produced by our method and post processed with Poisson surface reconstruction.
> 结果网格的可视化，由我们的方法生成并通过泊松曲面重建进行后处理。
> ```



Tags：

text-driven, perpetual view generation, 3D consistency, mesh representation, video generation

文本驱动、永久视图生成、3D 一致性、网格表示、视频生成





### 10、Scalable Adaptive Computation for Iterative Generation

Paper（论文地址）: 2022 [ Scalable Adaptive Computation for Iterative Generation](https://arxiv.org/abs/2212.11972)



Date：2022年12月23日

Summary：

The Recurrent Interface Network (RIN) is a neural net architecture that allocates computation adaptively to the input according to the distribution of information, allowing it to scale to iterative generation of high-dimensional data. RINs yield state-of-the-art image and video generation without cascades or guidance, while being domain-agnostic and up to 10x more efficient compared to specialized 2D and 3D U-Nets.

循环接口网络（RIN）是一种神经网络架构，它根据信息的分布自适应地将计算分配给输入，从而使其能够扩展到高维数据的迭代生成。 RIN 无需级联或引导即可生成最先进的图像和视频，同时与领域无关，并且与专门的 2D 和 3D U-Net 相比，效率提高了 10 倍。

Tags：

neural networks, iterative generation, image generation, video generation

神经网络、迭代生成、图像生成、视频生成



网友个人博客的阅读笔记：

* [Scalable Adaptive Computation for Iterative Generation | Qiang Zhang](https://zhangtemplar.github.io/iterative-generation/)
  * 这里的主要创新是将输入标记映射到更短的潜在变量。潜伏可以从先前的迭代（扩散过程）中初始化。因此，新方法可以实现与常规扩散方法相似的视觉保真度，但成本仅为常规扩散方法的 1/10。



Twitter 推特 社交媒体资讯：

* [AK on X: "Scalable Adaptive Computation for Iterative Generation abs: https://t.co/1HOY8H1UBe RIN, a NN architecture that allocates computation adaptively to the input according to the distribution of information, allowing it to scale to iterative generation of high-dimensional data https://t.co/FW5TfIbayy" / X](https://twitter.com/_akhaliq/status/1606104485390065664)

  * 迭代生成的可扩展自适应计算

    RIN，一种NN体系结构，根据信息的分布将计算自适应地分配到输入，从而使其扩展到迭代生成高维数据





### 11、Keras documentation: Denoising Diffusion Probabilistic Model

URL：https://keras.io/examples/generative/ddpm/

#### Introduction 介绍

Generative modeling experienced tremendous growth in the last five years. Models like VAEs, GANs, and flow-based models proved to be a great success in generating high-quality content, especially images. Diffusion models are a new type of generative model that has proven to be better than previous approaches.
生成建模在过去五年中经历了巨大的增长。事实证明，VAE、GAN 和基于流的模型等模型在生成高质量内容（尤其是图像）方面取得了巨大成功。扩散模型是一种新型的生成模型，已被证明比以前的方法更好。

We implement the Denoising Diffusion Probabilistic Models paper or DDPMs for short in this code example. It was the first paper demonstrating the use of diffusion models for generating high-quality images. The authors proved that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling that generates the best quality results.
我们在此代码示例中实现了去噪扩散概率模型论文（简称 DDPM）。这是第一篇演示如何使用扩散模型生成高质量图像的论文。作者证明，扩散模型的某些参数化揭示了训练期间多个噪声级别的去噪分数匹配以及采样期间退火的朗之万动力学的等价性，从而产生最佳质量的结果。

#### Conclusion 结论

We successfully implemented and trained a diffusion model exactly in the same fashion as implemented by the authors of the DDPMs paper. You can find the original implementation here.
我们成功地实现并训练了一个扩散模型，其方式与 DDPM 论文作者所采用的方式完全相同。您可以在这里找到原始实现。

The original implementation （原始实现）：https://github.com/hojonathanho/diffusion

GitHub（源代码地址）：https://github.com/keras-team/keras-io/blob/master/examples/generative/ddpm.py

View in Colab（在 Colab 中查看）：https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/ddpm.ipynb

Date：2022年12月13日

Summary：

This URL link  above is to the Keras documentation for the Denoising Diffusion Probabilistic Model (DDPM), which is a generative model that can be used for image and video generation. The page provides an overview of the model and its architecture, as well as code examples for training and using the model.

上面的 URL 链接是去噪扩散概率模型 (DDPM) 的 Keras 文档，DDPM 是一种可用于图像和视频生成的生成模型。该页面提供了模型及其架构的概述，以及用于训练和使用模型的代码示例。

Tags：

Keras, generative model, DDPM, image generation, video generation

Keras、生成模型、DDPM、图像生成、视频生成





### 12、Elai.io - AI Video Generation Platform

官方网站URL：https://elai.io/

温馨提示：在官方网站注册后，可以免费生成1分钟的视频。

Date：2022年12月5日

Summary：

Elai.io is an automated AI video generation platform that allows users to create personalized videos using digital avatars from text. The platform eliminates the need for a camera, studio, and greenscreen, making video creation quick and easy.

Elai.io 是一个自动化人工智能视频生成平台，允许用户使用文本中的数字化身创建个性化视频。该平台消除了对摄像机、工作室和绿幕的需求，使视频创建变得快速、轻松。

Features（主要功能特点）:

Multi-lingual voice cloning 多语言语音克隆：将您自己的声音克隆成 28 种语言，并利用它将您的所有想法变为现实。只需输入语音文本，头像就会用您的声音进行叙述。

Automated translations 自动翻译：一键式视频翻译将帮助您吸引全球观众。将您的视频翻译成 75 种可用语言。

AI Storyboard 人工智能故事板：适合学习和发展专业人士的完美内容创建工具。只需点击几下即可制作脚本，创建课程大纲，然后将其转变为引人入胜的视频。

Article-to-Video converter 文章到视频转换器：将您的博客文章从 URL 转换为带旁白的视频。将您博客文章的链接粘贴到亦来在线视频生成器中，让我们的平台完成剩下的工作。

PPTX-to-Video PPTX 转视频：上传 PPTX 文件，它将转换为视频幻灯片，创建具有可编辑组件的完全交互式演示文稿。

Avatar Dialogs 头像对话框：在幻灯片中添加 2 个头像以创建基于场景的学习视频。

Personalization at Scale 大规模个性化：大规模创建个性化视频营销活动，传递更具针对性的信息并与受众建立更牢固的关系。

Create your avatar 创建你的头像：创建您公司代表的数字化身，并使用 Elai 克隆他们的声音。

Screen Recorder 屏幕录像机：Elai.io 的屏幕录制功能允许用户轻松捕获并上传屏幕录制内容，以便在视频演示中使用它们。

等



Tags：

AI, video generation, digital avatars

人工智能、视频生成、数字化身



媒体评价：

* [Central Eastern Europe is a hub for exciting startups | WIRED UK](https://www.wired.co.uk/bc/article/google-for-startups-eastern-europe)
  * 利用先进的人工智能，Elai.io 的软件即服务工具允许用户仅使用文本轻松创建视频，将单词放入数字化身的嘴中。它提供了一个模板和动画库，支持超过 65 种语言，并且头像基于各种现实生活中的演员。

* [Elai.io - Product Information, Latest Updates, and Reviews 2023 | Product Hunt](https://www.producthunt.com/products/elai-io)

  * Elai.io 帮助您使用人工智能生成的人类从纯文本创建教育和营销视频内容。

* Forbes：2021年，初创公司Elai.io开发了一款将文本转换为视频内容的产品。作为输出，用户会收到由人工智能化身显示和发声的演示文稿。

  



### 13、Introducing a Practical Tool to Generate Images and Videos with AI

URL：https://twitter.com/tall/status/1587913529587249152

工具地址：https://stableboost.ai/

Date：2022年11月3日

Summary：

Tal Stramer has announced a new project, https://t.co/4hhV3KfNZi , which is a practical tool to generate images and videos with AI. The project was advised by Andrej Karpathy.

Tal Stramer 宣布了一个新项目https://t.co/4hhV3KfNZi ，这是一个利用 AI 生成图像和视频的实用工具。该项目由安德烈·卡帕蒂 (Andrej Karpathy) 提供建议。

> 以下援引自上述URL中项目作者Tal Stramer发言
>
> Tal Stramer：Why another generative AI service? To create the perfect image with AI, you often need to generate hundreds of variations, but existing tools only let you generate a few images at a time. Stableboost is built from the ground up to let you quickly generate a lot of images.
> 为什么需要另一种生成式人工智能服务？要使用 AI 创建完美的图像，您通常需要生成数百种变体，但现有工具只能让您一次生成几张图像。 Stableboost 是从头开始构建的，可让您快速生成大量图像。
>
> After generating many images for a prompt, Stableboost lets you interactively discover your favorites. It learns your preferences based on selecting a few images you like and recommends you similar images.
> 在为提示生成许多图像后，Stableboost 可以让您以交互方式发现您最喜欢的图像。它通过选择您喜欢的一些图像来了解您的偏好，并向您推荐类似的图像。
>
> Crafting a prompt to generate the images you want can involve a lot of trial and error. Stableboost speeds this up by letting you try out different variations of your prompt all at once.
> 制作提示来生成您想要的图像可能需要大量的试验和错误。 Stableboost 通过让您同时尝试不同的提示变体来加快速度。
>
> For fun, Stableboost also lets you generate hypnotic video animations that smoothly interpolate between text prompts.
> 为了好玩，Stableboost 还可以让您生成催眠视频动画，在文本提示之间平滑插入。
>
> Stableboost also supports many of the standard features people have come to expect from these types of services: Image-to-image, inpainting, face restoration, upscaling, etc.
> Stableboost 还支持人们期望从这些类型的服务中获得的许多标准功能：图像到图像、修复、面部恢复、升级等。
>
> Try Stableboost for free at [https://stableboost.ai](https://t.co/zWIG6ZwxsI) . You get 500 free credits every month and its 1 cent per image after that (the price is minimal so I can cover my costs). Enjoy!
> 访问 [https://stableboost.ai](https://t.co/zWIG6ZwxsI) 免费试用 Stableboost。您每月可以获得 500 个免费积分，之后每张图片 1 美分（价格很低，所以我可以支付我的费用）。享受！
>
> yes, you can upload an image to use as a reference and  set the style you want as the prompt. play with the guidance scale and initial image similarity settings to get good results. like this
> 是的，您可以上传图像作为参考，并设置您想要的样式作为提示。使用引导比例和初始图像相似度设置以获得良好的结果。像这样
>
> ![example](https://gitee.com/junhaoyu/work20221111/raw/master/img/202312191351540.jpg)

Tags：

AI, image generation, video generation, machine learning

人工智能、图像生成、视频生成、机器学习





### 14、Phenaki: A Model for Generating Videos from Text

URL：https://twitter.com/doomie/status/1577713150530445312

5/ phanaki最值得注意的功能是它具有随着时间的流逝而变化的提示来生成长视频的能力。这些可以将其视为故事，用户讲述并创建动态变化的场景。

Paper（论文地址）: 2022 [Phenaki: Variable Length Video Generation From Open Domain Textual Description](https://arxiv.org/abs/2210.02399)

https://pub-bede3007802c4858abc6f742f405d4ef.r2.dev/paper.pdf

我们提出 Phenaki，这是一种能够在给定一系列文本提示的情况下进行逼真视频合成的模型。由于计算成本、高质量文本视频数据数量有限以及视频长度可变，从文本生成视频尤其具有挑战性。为了解决这些问题，我们引入了一种新的因果模型来学习视频表示，该模型将视频压缩为离散标记的小型表示。该分词器及时使用因果注意力，这使得它可以处理可变长度的视频。为了从文本生成视频标记，我们使用以预先计算的文本标记为条件的双向屏蔽转换器。生成的视频令牌随后被去令牌化以创建实际视频。为了解决数据问题，我们演示了对大量图像文本对以及少量视频文本示例进行联合训练如何产生超出视频数据集中可用的泛化能力。与上一代视频相比 方法，Phenaki 可以在开放域中根据一系列提示（即时间可变文本或故事）生成任意长视频。据我们所知，这是第一次有论文研究根据时间变量提示生成视频。此外，所提出的视频编码器-解码器在时空质量和每个视频的标记数量方面优于目前文献中使用的所有每帧基线。 

项目地址：https://phenaki.github.io/

Date：2022年10月10日

Summary：

Dumitru Erhan introduces Phenaki, a model for generating videos from text with prompts that can change over time. The model is able to generate videos that can be as long as multiple minutes.

Dumitru Erhan 推出了 Phenaki，这是一种从文本生成视频的模型，其提示可以随时间变化。该模型能够生成长达数分钟的视频。

Tags：

AI, video generation, natural language processing

人工智能、视频生成、自然语言处理





### 15、Imagen Video: A New Text-Conditioned Video Diffusion Model

Imagen Video：A New Text-Conditioned Video Diffusion Model 一种新的文本条件视频扩散模型

URL：https://twitter.com/hojonathanho/status/1577712621037445121

兴奋地宣布Imagen Video，这是我们新的文本条件视频扩散模型，该模型生成1280x768 24fps HD视频！#ImagenVideo

 Imagen Video 的独特功能：例如生成不同艺术风格的视频、3D 理解以及文本渲染和动画。查看论文了解更多详细信息

借助渐进式蒸馏，Imagen Video 可以为每个子模型仅使用 8 个扩散步骤生成高质量视频。这大大加快了视频生成时间，大约提高了 18 倍。

另外，请查看 Phenaki，这是 Google Research 的一种用于文本到视频的补充方法，可以为一系列文本提示生成长而连贯的视频： https://phenaki.github.io 
我们期待结合 Phenaki 和 Imagen Video 的优势！

Paper（论文地址）: 2022 [Imagen Video: High Definition Video Generation with Diffusion Models](https://arxiv.org/abs/2210.02303)

https://imagen.research.google/video/paper.pdf

Date：2022年10月10日

Summary：

Jonathan Ho announces Imagen Video, a new text-conditioned video diffusion model that generates high-quality videos.

Jonathan Ho 宣布推出 Imagen Video，这是一种新的文本调节视频传播模型，可生成高质量视频。

Tags：

AI, video generation, deep learning

人工智能、视频生成、深度学习



媒体资讯：

* [Stable Diffusion、DreamFusion、Make-A-Video、Imagen Video 和下一步\_AI\_Luhui Hu\_InfoQ精选文章](https://www.infoq.cn/article/bkptwsyeidednaobvekv)

  * Google 的 Imagen Video 是一个基于视频扩散模型级联的文本条件视频生成系统。

    给定一个文本提示，Imagen Video 使用基本视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。

    它由七个子模型组成，分别执行文本条件视频生成、空间超分辨率和时间超分辨率。整个级联生成 1280×768（宽×高）的高清视频，每秒 24 帧，持续 128 帧 （~5.3 秒），大约 1.26 亿像素。

    Imagen Video 示例：“一束秋天的树叶落在平静的湖面上，形成文本‘Imagen Vide’。平滑。”生成的视频分辨率为 1280×768，持续时间为 5.3 秒，每秒 24 帧（来源：Imaged Video）

    原文链接：https://towardsdatascience.com/generative-ai-878909fb7868






## AI Funding Rounds

### 1、SeiSei.ai

SeiSei.ai is a text-to-video generation platform. SeiSei's algorithms can automatically change video lip sync and language, creating seamless and realistic videos.
SeiSei.ai 是一个文本到视频生成平台。 SeiSei 的算法可以自动更改视频口型同步和语言，创建无缝且逼真的视频。

Industry（所属行业）：Artificial Intelligence, SaaS, Video

Company Description（公司描述）：SeiSei.ai is a text-to-video generation platform.

Website（官方网站）：https://www.seisei.ai/

Announced Date（公布融资消息的日期）：2023年8月2日

Number of Funding Rounds（融资轮数）：1

Funding amount：undisclosed （融资金额未公开）

Crunchbase（CB数据库）：[SeiSei.ai - Funding, Financials, Valuation & Investors](https://www.crunchbase.com/organization/rgen-ai/company_financials)

Ethics Guidlines （道德准则）：https://www.seisei.ai/ethics

Official social media account（官方社交媒体账号）：

LinedIn:：https://www.linkedin.com/company/seisei-ai

YouTube:  [Demo - YouTube](https://www.youtube.com/playlist?list=PLqKwFgGmv-tN-ldy5zxNceST-_q_8UQF0)

Instagram： [SeiSei.ai (@seisei.ai\_official) · Instagram 照片和视频](https://www.instagram.com/seisei.ai_official/)

Facebook：https://www.facebook.com/seiseiai

Democratizing Generating Personalized Videos at scale.

大规模生成个性化视频。

使用步骤：

1、Upload 上传 Add Your Video and Audio Material 添加您的视频和音频材料

2、Input Your Script 输入您的脚本 Craft Your Message with Custom Variables 使用自定义变量制作您的消息

3、Generate 产生 Watch Our AI Craft Your Personalized Videos 观看我们的人工智能制作您的个性化视频

About（关于）：

##### Our mission 我们的使命

##### Make video campaigns easy for anyone 让任何人都能轻松开展视频营销活动

When individuals unite in pursuit of a shared vision, fueled by passion, the collective effort surpasses the sum of its parts. At SeiSei.ai, we hold a deep appreciation for our community, which includes both our dedicated team and our valued clients. Our culture is rooted in the conviction that the strength of ideas should eclipse considerations of title or hierarchy. Transparency serves as the cornerstone of our cultural ethos.
当个人在激情的推动下团结起来追求共同的愿景时，集体的努力就会超越各个部分的总和。在 SeiSei.ai，我们对我们的社区深表感谢，其中包括我们敬业的团队和尊贵的客户。我们的文化植根于这样的信念：思想的力量应该超越头衔或等级制度的考虑。透明度是我们文化精神的基石。



### Logging

20231223 Jack Lee 今天目标推进500词，最终推进到了15209词

昨天已经整理完了Ben's Bites 的 数据库1：Airtable - Grid view AI Project Tracker 人工智能项目追踪器的最新数据库

今天整理 Ben's Bites 的 数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库，于是在数据库2的表格里面用关键词**Video generation**进行过滤搜索（Filtered by Company Description 公司描述中含有**Video generation**）

今天主要增加了SeiSei.ai公司

20231222 Jack Lee 今天目标推进500词，最终推进到了14302词

今天主要增加了Imagen Video: A New Text-Conditioned Video Diffusion Model

20231221 Jack Lee 今天目标昨天已完成

20231220 Jack Lee 今天目标推进500词，今天准备的是明天的进度，最终推进到了13698词

今天主要增加了Phenaki: A Model for Generating Videos from Text

20231219 Jack Lee 今天目标推进1000词，因为1221有事情，所以提前准备一天进度，减少明天的压力，最终推进到了12972词

今天主要增加了Elai.io - AI Video Generation Platform、Introducing a Practical Tool to Generate Images and Videos with AI，昨晚从微信公众号看到数字生命卡兹克消息，加入了他的微信群，补充了PIKA即将全面公测的最新消息

20231218 Jack Lee 今天目标推进500词，最终推进到了10443词

20231217 Jack Lee 今天目标推进500词，最终推进到了13685词，然后开始根据阳志平老师之前给的建议，以确保内容的质量不会受到牺牲。保证信息的准确性、分析的深度以及对读者的实际用处为指导，精简一些文字，最终精简到了9745词

今天主要增加了Scalable Adaptive Computation for Iterative Generation和Tags标签的双语格式，参考 OpenMindClub/awesome-chatgpt仓库的首页README.md 统一了本文档中目前存在的维基百科相关词条和论文地址的格式，论文地址部分修改格式为论文提交年份+论文标题，精简了Runway's Gen-2的媒体资讯部分、WonderJourney的描述部分、阿里Animate Anyone、字节跳动的MagicAnimate、微软的GAIA、GitHub - arpitbansal297/Universal-Guided-Diffusion媒体资讯的部分文字

20231216 Jack Lee 今天目标推进500词，最终推进到了13133词

今天继续整理**Ben's Bites**两个数据库中的每天更新的人工智能项目追踪器数据库中的内容

今天主要增加了SceneScape: Text-Driven Consistent Scene Generation

20231215 Jack Lee 今天目标推进500词，最终推进到了12288词

今天继续整理**Ben's Bites**两个数据库中的每天更新的人工智能项目追踪器数据库中的内容

今天主要增加了GitHub - arpitbansal297/Universal-Guided-Diffusion

20231214 Jack Lee 今天目标推进500词，最终推进到了11231词

今天受到首届东木人生发展挑战赛微信群开智校友（川子同学和jack同学）启发，然后主动搜索到了公众号数字生命卡兹克的《盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来》一文。明天继续整理**Ben's Bites**两个每天更新的数据库中的内容

今天主要增加了阿里的Animate Anyone、字节跳动的MagicAnimate**、**微软的GAIA，还补充了一下pika部分的相关内容。

20231213 Jack Lee 今天目标推进500词，最终推进到了4509词

今天主要增加了Gen-1部分和Gen-1 Explained、Gen-2 Explained，Text-to-Video Tool

主要思路是借助OpenMindClub/awesome-chatgpt仓库的首页README.md 中的

- [Find AI Tools Using AI](https://theresanaiforthat.com/?message=subscribed) - AI tools. Updated daily.
  使用 AI 查找 AI 工具 - AI 工具。每日更新。

进一步明确关键词为**Video generation**，配合阳志平老师提供的方法：从自己能拿到的最优质的公开数据，掌握的最优质的信息入手，我由此联想到我拥有**Ben's Bites**两个每天更新的数据库的访问权限，于是在数据库1表格里面用关键词**Video generation**进行过滤搜索（Filtered by Tags 标签中含有**Video generation**），然后完成了今天的500词。

We have two databases that are updated every day;
我们有两个每天更新的数据库；

All 10k+ links we’ve covered, easily filterable
我们涵盖的所有 10k+ 链接都可轻松过滤

6k+ AI company funding rounds from Jan 2022, including investors, amounts, stage etc
2022年1月起超过6k+轮AI公司融资，包括投资者、金额、阶段等

数据库1：Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。可过滤和可搜索。包括相关的筹款和投资者数据。

> It includes all links mentioned in the emails, from the first issue. It'll keep updating over time.
>
> 它包括电子邮件中提到的从第一期开始的所有链接。它将随着时间的推移不断更新。

https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库 https://airtable.com/appLPuy8wEZ8dbiHI/shrNYsG0mStB7NY06/tblBbns8QETdkqrXC

> The database contains of all AI company funding rounds, the amounts, location, investors and much more.
>
> We populate the information where possible but as with private company data, it may be inaccurate or missing, unfortunately. We verify the data from Crunchbase, Pitchbook and other sources where possible.
>
> 该数据库包含所有人工智能公司的融资回合、金额、地点、投资者等信息。
>
> 我们尽可能填充信息，但遗憾的是，与私人公司数据一样，这些信息可能不准确或丢失。我们尽可能从 Crunchbase, Pitchbook 和其他来源核实数据。

20231212  Jack Lee 今天搬运OpenMindClub/awesome-chatgpt仓库的首页README.md&README.zh-cn.md文件并稍作初步修改，然后在List  about the current collection of AI video projects.md中继续增加字数，增加了元资源中维基百科中文本到视频模型词条和生成式人工智能词条，进一步完善了Pika、Gen-2、WonderJourney的介绍 目前字数2771词

PS：本文档中出现的OpenMindClub/awesome-chatgpt仓库统一特指开智学堂的GitHub仓库 [OpenMindClub/awesome-chatgpt: ⚡ Everything about ChatGPT](https://github.com/OpenMindClub/awesome-chatgpt/tree/main#general)

20231211  Jack Lee init 今天推进初稿500词 20000字/42天=477字/天