

## Awesome清单链接 ：[Awesome-AI-Video-Projects/List about the current collection of AI video projects.md at main · freejacklee/Awesome-AI-Video-Projects](https://github.com/freejacklee/Awesome-AI-Video-Projects/blob/main/List%20%20about%20the%20current%20collection%20of%20AI%20video%20projects.md)

* [Awesome-AI-Video-Projects/List about the current collection of AI video projects.md at main · freejacklee/Awesome-AI-Video-Projects] https://github.com/freejacklee/Awesome-AI-Video-Projects/blob/main/List%20%20about%20the%20current%20collection%20of%20AI%20video%20projects.md





## 仓库地址：freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects

* [freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects] https://github.com/freejacklee/Awesome-AI-Video-Projects





## 提交内容：List  about the current collection of AI video projects.md

This is an awesome GitHub list of information about the current collection of AI video projects

List  about the current collection of AI video projects





## 元资源

* [Wikipedia Text-to-video model](https://en.wikipedia.org/wiki/Text-to-video_model)

  * A **text-to-video model** is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning) model which takes as input a [natural language](https://en.wikipedia.org/wiki/Natural_language) description and produces a [video](https://en.wikipedia.org/wiki/Video) matching that description.[[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)
    文本到视频模型是一种机器学习模型，它将自然语言描述作为输入并生成与该描述匹配的视频。 [[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)

    Video prediction on making objects realistic in a stable background is performed by using [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) for a sequence to sequence model with a connector [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) encoding and decoding each frame pixel by pixel,[[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) creating video using [deep learning](https://en.wikipedia.org/wiki/Deep_learning).[[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)
    通过使用循环神经网络进行序列到序列模型的视频预测，使对象在稳定的背景下具有连接器卷积神经网络逐像素编码和解码每个帧， [[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) 使用深度学习创建视频。 [[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)

* [Wikipedia Generative artificial intelligence - Wikipedia](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)
  
  * Video 视频
    * Generative AI trained on annotated video can generate temporally-coherent video clips. Examples include Gen-1 and Gen-2 by [Runway](https://en.wikipedia.org/wiki/Runway_(company))[[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) and Make-A-Video by Meta Platforms.[[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)
      经过带注释视频训练的生成式人工智能可以生成时间连贯的视频剪辑。示例包括 Runway [[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) 的 Gen-1 和 Gen-2 以及 Meta Platforms 的 Make-A-Video。 [[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)



## AI generated video tool

### 1、Pika

Website: https://pika.art 
Discord: http://discord.gg/pika 
About: https://pika.art/about

官方社交媒体：

Twitter 推特 ： [Pika (@pika\_labs) / X](https://twitter.com/pika_labs)

小红书 官方号：Pika AIvideo (小红书号7027752781）

创始人之一 郭文景女士的推特账号 [Demi Guo (@demi\_guo\_) / X](https://twitter.com/demi_guo_?lang=en)

来自公众号 数字生命卡兹克的用户测评：*[【全网首发】PIKA1.0上手评测 - 你就是传奇](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660666&idx=1&sn=0e9e2a11d5c06cd512479d35ca84bf3f&sharer_shareinfo=d3d03bfa8311534acd00b7003123ed3c&sharer_shareinfo_first=d3d03bfa8311534acd00b7003123ed3c#rd)



我的个人体验：20231210已申请加入waitlist，暂未获得邀请资格，继续期待。我在Pika Labs的Discord上用一张图片生成的视频中人物的脸会变形。2023年12月22日 11:05  2023年12月25日 17:10邮件和discord都获得了体验资格，个别试验效果还是差强人意。

温馨提示：

1、小红书app 搜 Jessie_Ma(小红书号946999884），12月27日早上06:37 Jessie_Ma在小红书上发布消息 宣布 Pika1.0已全面开放
Pika的圣诞礼物是给大家全面开放1.0的Access啦！排队结束

![1227pika全面开放](https://gitee.com/junhaoyu/work20221111/raw/master/img/202312270914102.jpeg)

![pika](https://gitee.com/junhaoyu/work20221111/raw/master/img/202312270914981.jpeg)

2、公测之后使用指南如下：

来自 数字生命卡兹克

* [我用了2周PIKA1.0后，总结了10个宝藏使用技巧 - 建议收藏公测后用](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660789&idx=1&sn=756a1ca3f9c3fb7b9edf9f248ee9863e&sessionid=1703639707&subscene=227&clicktime=1703639711&enterid=1703639711#rd)







趣闻：

* [哈佛斯坦福学霸女儿创业AI项目走红全球，老爸公司两个涨停：仅是父女，没投钱](https://baijiahao.baidu.com/s?id=1784049635636388017&wfr=spider&for=pc)

  * A股又现“女儿概念股”。

    近日，人工智能应用Pika走红，该应用创始人的父亲实控的A股上市公司信雅达也被带火，12月1日开盘继续一字涨停，拿下两连板。

    11月30日，信雅达科技股份有限公司（信雅达，600571）发布澄清公告称，近日，信雅达关注到媒体关于“视频生成应用Pika”的相关报道，为避免相关信息对广大投资者造成误导，现予以澄清说明。

    信雅达表示，Pika开发团队创始人之一郭文景系公司实际控制人郭华强的女儿。除上述关系外，公司与Pika无其他关系。

    公告强调，截至目前，郭文景未在公司担任任何职务，公司未投资Pika，也未与Pika有任何业务往来。

    此事源于人工智能产品Pika1.0在网络爆火。




媒体资讯：

* [我用了2周PIKA1.0后，总结了10个宝藏使用技巧 - 建议收藏公测后用](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660789&idx=1&sn=756a1ca3f9c3fb7b9edf9f248ee9863e&sharer_shareinfo=a5e572e39b7ac90935b67162d22c0305&sharer_shareinfo_first=a5e572e39b7ac90935b67162d22c0305#rd)
  * 之前的PIKA1.0首发评测，可以看我的这篇文章：【全网首发】PIKA1.0上手评测 - 你就是传奇
    关于PIKA的使用技巧，这次我自己总结了10个，个人感觉还挺有用的，在PIKA即将全面公测之际，分享给大家，希望对大家后面使用PIKA1.0，会有一些帮助。

* [Pika爆火，但AI视频还没到「GPT时刻」-36氪](https://36kr.com/p/2558743578239112)

  * 某种程度上，这其实反映出了一个趋势：**比起文生图的竞争，在更高门槛的AI视频，创业公司寻求商业化的意愿更强烈。**

    产生上述焦虑的原因也并不难理解。 

    **一是算力的掣肘，视频领域对算力需求更高。** Pika联创就曾举过一个例子：“对于 Stable Diffusion，有人可能用8张A100就能从头开始学习，并得到不错的结果。但对于视频模型，用8张A100可能不够了，可能无法训练出一个好的模型。” 

    她甚至坦言，开源社区可能没有足够的算力来训练新的视频模型，除了一些大公司开源模型外，普通开源社区很难进行探索性工作。 

    **二是竞争环境的激烈。** 在AI视频产品层面，一方面正如上文所梳理的，头部科技巨头基本都已入局，只是产品尚未全面公测。另一方面，也包括了如Adobe此类面向专业级用户的老牌软件巨头和如已有先发优势的Runway。 

    还有一类则是HeyGen、Descript、CapCut类的轻量化视频制作产品。 

    大型科技公司具备算力优势，特别在是目前尚未有巨头明确开源路线（只有Stability AI发布了开源生成式视频模型Stable Video Diffusion）。而Adobe此类企业的优势在于AI视频功能和原有业务形成有力的协同，形成更高频的使用。Adobe此前也收购了一家AI视频领域的初创公司Rephrase.ai。 

    而轻量化的视频制作产品本身面向的是非专业人群，这意味着能否以差异化优势快速圈中人群，占据心智成为关键。 

    套用一句老生常谈，人们对技术的态度永远是高估短期，低估长期，AI视频也并不例外。 


* [Pika on X: "We’re beginning to let people in off the waitlist, and want to welcome our first Pika 1.0 users! Here’s how to start creating with text-to-video. Sign up at https://t.co/nqzjGy82Lx https://t.co/GXVx9WhM2i" / X](https://twitter.com/pika_labs/status/1734311655771673040)

  * 我们开始让人们进入候补名单，并希望欢迎我们的第一个Pika 1.0用户！

    这是开始使用文本到视频创建的方法。

    请注册https://pika.art/waitlist

* [AI生成视频工具Pika爆火，估值超2亿美元-虎嗅网](https://m.huxiu.com/article/2361484.html#:~:text=%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%8D%8E%E4%BA%BA%E5%AD%A6%E7%94%9F%E9%80%80%E5%AD%A6%E5%88%9B%E5%8A%9E,%E7%BC%96%E8%BE%91%E5%92%8C%E9%87%8D%E6%96%B0%E6%9E%84%E6%83%B3%E5%9C%BA%E6%99%AF%E3%80%82&text=%F0%9F%92%A5%20Pika%201.0%E4%BD%BF%E7%94%A8AI,%E7%AE%80%E5%8D%95%E4%B8%94%E9%A3%8E%E6%A0%BC%E5%A4%9A%E5%8F%98%E3%80%82)
  * 斯坦福华人学生退学创办的AI视频生成工具Pika 1.0正式推出，估值超过2亿美元。 该工具可以通过文字、图片和视频生成高质量的各种风格视频，并且支持用户上传视频片段进行编辑和重新构想场景。 💥 Pika 1.0使用AI模型生成非常贴近生动的视频，使用简单且风格多变。

* [斯坦福华人博士文生视频Pika 1.0爆火，4人公司估值2亿，OpenAI联创参投-36氪](https://36kr.com/p/2539021165094660) 

  * [新智元](https://36kr.com/user/574825230) 发表于2023-11-29 15:30

  * Runway Gen-2最强竞品Pika，暌违半年忽然放出大招——Pika 1.0正式发布！

    仅成立六个月，Pika就结束了测试版，正式发布了第一个产品，能够生成和编辑3D动画、动漫、卡通和电影。


* [斯坦福华人博士文生视频Pika 1.0爆火！4人公司估值2亿，OpenAI联创参投](https://mp.weixin.qq.com/s?__biz=MzU0OTkwNTM2Mw==&mid=2247653802&idx=1&sn=d8597cc271043b01541ff96aa561fc22&sharer_shareinfo=2d57f4d178c2f63c3692057c22223146&sharer_shareinfo_first=2d57f4d178c2f63c3692057c22223146#rd)
  * 在获取Pika 1.0试用资格之前，和Midjourney一样，用户现在通过Discord获取Pika Labs的视频生成服务。用户只需在聊天框输入文字，比如「一个机器人在日落沙滩上行走」，就能收到一个由AI生成的视频。
    周二，Pika把这一体验带到了网页上，面向更广泛的主流群体，让他们可以在编辑视频、自定义物体。这里还有一段，Pika创意总监前几天放出的，用Pika文本转视频AI功能制作的「3D动画预告片」，效果萌到爆。
* [4个人，Pika估值10亿](https://mp.weixin.qq.com/s?__biz=Mzk0ODUwNjUxNQ==&mid=2247484633&idx=1&sn=bf28878224f5ef7205214401a7d5669a&sharer_shareinfo=9978076d64328890c7eb7018395ec70d&sharer_shareinfo_first=9978076d64328890c7eb7018395ec70d#rd)
  * 要了解关于Pika的最新信息，请关注X上的@pika_labs，加入pika的Discord社区并在这里访问我们的测试版产品，并在https://pika.art加入新Pika 1.0的等待名单。
* [中国天才少女硅谷创立AI公司，半年估值超10亿](https://mp.weixin.qq.com/s?__biz=MTI3NTQ1MTY0MQ==&mid=2650603935&idx=1&sn=368bd29cbf96b8095e5541ae6689b229&sharer_shareinfo=eda2394bbef20b16a62fb59bd91e59ac&sharer_shareinfo_first=eda2394bbef20b16a62fb59bd91e59ac#rd)
  * 目前，Pika1.0正式的网页版需要排队预约，尚未有用户实际测评过。有人借此质疑，横空出世的Pika一夜爆红，是否为一场营销骗局？毕竟，在11月之前，Pika还只是一个无名之辈。事实上，Pika的第一个版本今年4月下旬就在Discord上进行了公测。7月，在Discord正式推出服务器，并在几个月时间内收获了50万用户。由于Pika团队精简，寄生在Discord平台，能够最大限度地减少开发量。
    最初，Pika只支持文本生视频，后来逐渐支持图片转视频、相机控制、文字和Logo嵌入视频中等。Pika1.0宣传片中的许多功能，目前Discord上的版本并不支持，只能等网页版开放测评后验证。Pika也并非第一次在众人前亮相。今年11月初，《流浪地球3》的发布会上，电影工业化实验室G!Lab官宣成立。郭帆导演介绍了一批战略合作的科技公司，包括商汤科技、小米、华为等，还有Pika Labs。至今，成立仅6个月的Pika已经完成了三轮融资，总金额5500万美元，估值超10亿元人民币。投资人阵容也可谓豪华——包括OpenAI董事会成员Adam D'Angelo与前特斯拉AI总监Andrej Karpathy、前Github CEO Nat Friedman、YC合伙人Daniel Gross，以及硅谷著名投资人Elad Gil等。
* [Pika, which is building AI tools to generate and edit videos, raises $55M | TechCrunch --- Pika 正在构建用于生成和编辑视频的 AI 工具，筹集了 5500 万美元 | TechCrunch](https://techcrunch.com/2023/11/28/pika-labs-which-is-building-ai-tools-to-generate-and-edit-videos-raises-55m/)




@阑夕
AI视频生产，新兴产品Pika和老牌产品Runway
的对比，同一张图片、同样的设置，可以看得出来已经各有千秋了。

来自阑夕的微博视频号 https://weibo.com/tv/show/1034:4977370101383191?from=old_pc_videoshow





### 2、Runway Gen-2

Website: https://research.runwayml.com/gen2

Discord: https://discord.com/invite/runwayml

About: https://research.runwayml.com/about

Twitter 推特 官方社交媒体：[Runway (@runwayml) / X](https://twitter.com/runwayml)

Date：2023年6月8日

* [Runway on X: "Gen-2: Text to Video is here. Learn the basics with today's Runway Academy. Watch the video: https://t.co/E5HEKrJpNX" / X](https://twitter.com/runwayml/status/1666793633524195328)



Summary：

Runway's Gen-2 allows users to create videos in any style using Text to Video generation. Runway 的 Gen-2 

允许用户使用文本到视频生成功能创建任何风格的视频。



Tags：

AI, video generation, Runway

人工智能、视频生成、Runway



Gen-2 Explained：

Not too long ago, runway pushed the boundaries of generative Ai with Gen One a video to video model that allows you to use words and images to generate new videos out of existing ones in the week since launching, the model has constantly gotten better temporal consistency, better fidelity better results and as more and more people gained access, we unlocked entirely new use cases and displays of creativity and today we're excited to announce our biggest unlock yettext to Video with Gen Two now. You can generate a video with nothing but words, no driving video no input image gen 2 represents yet another major research milestone and another monumental step forward for generative Ai with Gen 2, anyone anywhere can suddenly realize entire worlds, animations stories anything you can imagine gen two coming, very soon to https://runwayml.com/

不久前，runway通过Gen One 突破了生成式AI的界限，这是一个视频到视频模型，允许您使用文字和图像从现有视频中生成新视频，自推出以来的一周内，该模型不断获得更好的时间一致性，更好的保真度，更好的结果，并且随着越来越多的人获得访问权限， 我们解锁了全新的用例和创造力展示，今天我们很高兴地宣布，我们迄今为止最大的解锁版本是第二代视频。你可以生成一个只有文字的视频，没有驾驶视频，没有输入图像，第二代代表了另一个重要的研究里程碑，也是生成式人工智能向前迈出的又一重大一步，第二代，任何地方的任何人都可以突然意识到整个世界，动画故事、任何你能想象到的第二代即将到来，很快就会 https://runwayml.com/

个人体验：20231212尝试了一次，用一张弹古筝的女子图片和一句简短的文字生成视频，生成的视频中随着时间流逝人物面部会有点变形。

* [What is Gen-2 AI and How to Use It? A Step-by-Step Guide --- 什么是第二代人工智能以及如何使用它？分步指南](https://ambcrypto.com/blog/what-is-gen-2-and-how-to-use-it-a-step-by-step-guide/)

  * Gen-2 AI is the second generation of Runway’s AI software, which focuses on generating videos from scratch using text descriptions, images, or existing video clips. 
    Gen-2 AI 是 Runway 的第二代 AI 软件，专注于使用文本描述、图像或现有视频剪辑从头开始生成视频。

    This cutting-edge technology opens a realm of possibilities for content creators, allowing them to craft distinctive and captivating videos without resorting to costly equipment or lengthy procedures.
    这项尖端技术为内容创作者打开了一个可能性的领域，使他们能够制作独特且引人入胜的视频，而无需诉诸昂贵的设备或冗长的程序。

    The Gen-2 AI model is designed to help users create dreamy videos by harnessing the power of AI and generative algorithms, allowing for an unparalleled level of customization and fidelity in the final output. 
    Gen-2 AI 模型旨在帮助用户利用 AI 和生成算法的力量来创建梦幻视频，从而在最终输出中实现无与伦比的定制化和保真度。

    With Gen-2 AI, the days of being limited by available footage or budget constraints are numbered, as this innovative technology brings endless creative possibilities to the table.
    有了第二代人工智能，受可用镜头或预算限制的日子已经屈指可数了，因为这项创新技术带来了无限的创意可能性。

* [全面开放，无需排队，Runway视频生成工具Gen-2开启免费试用](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650885185&idx=3&sn=d8042976cd29c23e0c0ba8ee1923a645)
  * Runway 宣布，Gen-1 和 Gen-2 已经彻底开放，任何人都可以注册一个账号免费尝试。生成的视频长度为 4 秒，每秒消耗 5 个积分，利用免费额度可以生成二十几个视频。如果免费积分耗尽，付费标准为 0.01 美元 / 积分，也就是生成一个视频需要 0.2 美元。*2023-07-25 13:24* *机器之心 发表于北京*

* [图像涂哪就动哪，Gen-2新功能“神笔马良”爆火，网友：急急急-36氪](https://36kr.com/p/2515454214115330)

  

#### Runway Gen-1

Introducing Gen-1: A New AI Model for Video Generation

Website: https://research.runwayml.com/gen1

About: https://research.runwayml.com/about

Paper: 2023 [Structure and Content-Guided Video Synthesis with Diffusion Models](https://arxiv.org/abs/2302.03011)

Date：2023年2月7日

* [Runway on X: "Today, Generative AI takes its next big step forward. Introducing Gen-1: a new AI model that uses language and images to generate new videos out of existing ones. Sign up for early research access: https://t.co/7JD5oHrowP https://t.co/4Pv0Sk4exy" / X](https://twitter.com/runwayml/status/1622594989384519682)



Summary：

Runway has announced the launch of Gen-1, a new AI model that uses language and images to generate new videos out of existing ones. Early research access is available by signing up on their website. 

Runway 宣布推出 Gen-1，这是一种新的人工智能模型，可以使用语言和图像从现有视频中生成新视频。通过在其网站上注册即可获得早期研究访问权限。



Tags：

AI, video generation, Generative AI, Runway

人工智能、视频生成、生成式人工智能、Runway



Gen-1 Explained：

Gen 1 is able to realistically and consistently apply the composition and style of an image or text prompt to the target video allowing you to generate new video content using an existing video.We call this approach video to video, and we're incredibly excited to share a few early use casesstylization mode, transfer the style of any image or prompt to every frame of your video storyboard mode, turn mockups into fully stylized and animated rendersmask mode, isolate subjects in your video and modify them with simple text prompts.Render mode, turn untextured renders into realistic outputs by applying an input imageor prompt.
To realizing the future of storytelling.

Gen 1 能够逼真且一致地将图像或文本提示的构图和样式应用于目标视频，从而允许您使用现有视频生成新的视频内容。我们将这种方法称为视频到视频，我们非常高兴地分享一些早期的用例风格化模式，将任何图像或提示的样式传输到视频故事板模式的每一帧，将模型转换为完全风格化和动画渲染蒙版模式，隔离视频中的主题并使用简单的文本提示对其进行修改。渲染模式，通过应用输入图像或提示将无纹理渲染转换为逼真的输出。
实现讲故事的未来。





### 3、WonderJourney

项目及演示：https://kovenyu.com/wonderjourney/
论文：2023 [WonderJourney: Going from Anywhere to Everywhere](https://arxiv.org/abs/2312.03884)
GitHub：https://github.com/KovenYu/WonderJourney（Coming soon!）



[ [小互 on X: "WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。 它能够根据用户提供的文本描述或图片，自动生成一系列3D场景的连续画面。 这些场景不仅多样化，而且彼此之间还能紧密衔接，形成一种虚拟的“奇妙旅程”场景。 而且你只需要输入一段描述或上传一张图片即可... 主要功能特点：… https://t.co/gptrWSyWBz" / X](https://twitter.com/xiaohuggg/status/1733779657722622449)](https://twitter.com/xiaohuggg/status/1733779657722622449)

* 发表于2023-12-10 17:24 

* WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。

  它能够根据用户提供的文本描述或图片，**自动生成一系列3D场景的连续画面。**

  这些场景不仅多样化，而且**彼此之间还能紧密衔接**，形成一种**虚拟的“奇妙旅程”场景**。

  **而且你只需要输入一段描述或上传一张图片即可...**

  

  **主要功能特点：**

  与之前专注于单一场景类型的视图生成工作不同，WonderJourney从任何用户提供的位置（通过文本描述或图像）开始，生成一系列多样化但连贯相连的3D场景。

  更具体的主要功能特点详见上述推特

  

  **工作原理：**

  该框架利用大语言模型（LLM）生成场景的文本描述，一个由文本驱动的点云生成管道来制作引人入胜且连贯的3D场景序列，以及一个视觉语言模型（VLM）来验证生成的场景。

  更具体的工作原理详见上述推特



[WonderJourney谷歌合作的 3D 场景生成, 带你走进“奇妙旅程”](https://mp.weixin.qq.com/s?search_click_id=12501261693348582618-1702361382506-3224040202&__biz=Mzg2ODk4MDUxOQ==&mid=2247485556&idx=1&sn=b71b69562cab374580961cd61e05b976#rd)

* 给定诗歌或故事提要等一系列文本描述,WonderJourney也可以生成古诗词场景。





### 4、Text-to-Video Tool（Create mini AI videos from text）

URL： [TextToVideo | Create videos from text](https://text-to-video.vercel.app/)

Newsletter Post Title：ChatGPT business ideas with a billionaire

Newsletter Post URL：https://bensbites.beehiiv.com/p/government-bans-will-ai-emerge-victorious

Date：2023年4月3日



Summary：

This link leads to a website that offers a text-to-video tool. Users can input text and the tool will generate a video based on the content. The website also offers customization options for the video's appearance and background music.

此链接指向一个提供文本转视频工具的网站。用户可以输入文本，该工具将根据内容生成视频。该网站还提供视频外观和背景音乐的自定义选项。



Tags：

text-to-video, video generation, AI tool

文本转视频、视频生成、AI工具



来自 Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

* [Text to Video AI - Product Information, Latest Updates, and Reviews 2023 | Product Hunt --- 文本转视频 AI - 2023 年产品信息、最新更新和评论 |产品搜索](https://www.producthunt.com/products/text-to-video-ai)

  * Text to Video AI 文本转视频人工智能

  * Launched on March 31st, 2023

    Text to Video is my latest project, which allows you to create videos using AI. Currently AI videos are in their "monstrous stage", just like Dalle 2 MINI a while back. The project seeks that people can have a first approach to text-to-video.
    文本到视频是我的最新项目，它允许您使用人工智能创建视频。目前人工智能视频正处于“怪物阶段”，就像不久前的 Dalle 2 MINI 一样。该项目旨在让人们能够拥有第一种将文本转为视频的方法。





* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * **阿里的Animate Anyone**

    **字节跳动的MagicAnimate**

    **微软的GAIA**

* [阿里、字节悄悄上线AI神器，让梅西跳舞不在话下-36氪](https://36kr.com/p/2549019770755460)

* [阿里大战字节！Animate Anyone vs Magic Animate！AI短视频领域的学术之争！背后技术谁更强？ - YouTube](https://www.youtube.com/watch?v=L-iXbg-GTXk)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下 | 论文频道 | 领研网](https://www.linkresearcher.com/theses/6e61b037-0fbd-4e39-a406-baef6e27afed)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下-36氪](https://36kr.com/p/2543100034213636)

### 5、阿里的Animate Anyone

阿里推出了Animate Anyone，该项目由阿里巴巴智能计算研究院开发，你只需提供一个静态的角色图像（包括真人、动漫/卡通角色等）和一些动作、姿势（比如跳舞、走路），便可将其动画化，同时保留角色的细节特征（如面部表情、服装细节等）。

阿里项目：https://github.com/HumanAIGC/AnimateAnyone

**项目（Animate Anyone官网）地址：**https://humanaigc.github.io/animate-anyone/

阿里论文：2023  [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117)
阿里相关论文发布于2023年11月28日

* [有媒体报道，最近很火... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NwMlp5iDZ?type=repost)

  * 宝玉xp

    23-12-12 13:55
    有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的
    
    转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练 

    微博文字部分略，详见上面的网页链接

    ——来源：* [Alibaba's 'Animate Anyone' Is Trained on Scraped Videos of Famous TikTokers](https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

* [阿里巴巴智能计算研究院打造Animate Anyone：一种革命性的角色动画技术 | ATYUN.COM 官网-人工智能教程资讯全方位服务平台](https://www.atyun.com/57976.html)

  * 文字部分略，详见上面的网页链接

    文章来源：https://analyticsindiamag.com/this-new-ai-tool-could-mark-the-beginning-of-the-end-for-tiktok-and-instagram-influencers/

Animate Anyone在Huggingface上的在线测试地址：暂未发现已经开源 * [AnimateAnyone (AnimateAnyone)](https://huggingface.co/AnimateAnyone)



### 6、字节跳动的MagicAnimate

Magic Animate是一项开创性的开源项目，简化了动画创作，允许您从单个图像和动态[视频](https://www.yjpoo.com/shipinsucai/)制作动画视频，简单来说，给定一张参考图像和一个姿态序列（视频），它可以生成一个跟随姿态运动，并保持参考图像身份特征的动画视频。由新加坡国立大学的Show Lab和字节跳动打造。

Magic Animate在所有舞蹈视频解决方案中提供最高的一致性，但是Magic Animate 面部和手部可能会出现一些扭曲。默认配置可能会导致从动漫到写实主义的风格转变，尤其是在视频中的面部。将动漫风格应用于默认的DensePose驱动视频也会影响身体比例。

**Magic Animate官网地址：**www.magicanimate.org

字节项目：https://github.com/magic-research/magic-animate

字节论文：2023  [MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://arxiv.org/abs/2311.16498)

字节相关论文发布于2023年11月27日

MagicAnimate在Huggingface上的在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate

* [抖音跳舞不用真人出镜，一张照片就能生成高质量视频-虎嗅网](https://m.huxiu.com/article/2389861.html?f=rss)

  * 这就是来自新加坡国立大学和字节跳动最新的一项研究，名叫**MagicAnimate**。

    它的作用简单来说可以总结为一个公式：一张**图片** + 一组**动作** = 毫无违和感的**视频**。

* [新加坡国立大学和字节... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NvETcaeD1)

  * 宝玉xp

    23-12-5 05:06
    来自 微博视频号
    新加坡国立大学和字节跳动联合推出的MagicAnimate，可以用一张照片加上骨骼动画制作小姐姐跳舞视频。这和前几天阿里推出的 AnimateAnyone 网页链接 很像。
    
    MagicAnimate：通过扩散模型创造时间连贯的人像动画，并提供了 Gradio 演示

    本地演示链接:* [magic-research/magic-animate: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://github.com/magic-research/magic-animate#-gradio-demo?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

    本论文深入探讨了人像动画任务——其核心目标是制作一段视频，展示一个特定人物身份按照既定动作序列进行动作。中间文字略，详见上面的网页链接，经验证实，我们的方法在两个基准测试中均优于现有的基准方法。特别是在挑战性极高的 TikTok 舞蹈数据集上，我们的方法在视频真实度方面比最强基线提高了超过 38%。我们将公开代码和模型。

    论文：https://arxiv.org/abs/2311.16498
    项目首页：https://showlab.github.io/magicanimate/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38
    代码：https://github.com/magic-research/magic-animate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38 宝玉xp的微博视频 https://weibo.com/tv/show/1034:4975452565995522?from=old_pc_videoshow
    在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38
    
    #微博新知#

阿里、字节两公司在Github上的开源文件还在不断更新中。



### 7、微软的GAIA

项目地址：https://microsoft.github.io/GAIA/       12月14日访问该网页显示404错误

Paper：2023 [GAIA: Zero-shot Talking Avatar Generation](https://arxiv.org/abs/2311.15230)

GitHub：https://github.com/microsoft/GAIA  12月14日访问该网页显示404错误

* [How to Create Talking Virtual Characters with Microsoft GAIA - YouTube](https://www.youtube.com/watch?v=mwsfS0dq_bc)

* [Stunning Breakthroughs in AI Creativity! - YouTube](https://www.youtube.com/watch?v=yfxZKoTOka0)

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制 | 机器之心](https://www.jiqizhixin.com/articles/2023-12-04-4)

  * 视频 PS 可以灵活到什么程度？最近，微软的一项研究提供了答案。

    在这项研究中，你只要给 AI 一张照片，它就能生成照片中人物的视频，而且人物的表情、动作都是可以通过文字进行控制的。比如，如果你给的指令是「张嘴」，视频中的人物就会真的张开嘴。

    。。。。。。

    这项研究名叫 GAIA（Generative AI for Avatar，用于虚拟形象的生成式 AI），其 demo 已经开始在社交媒体传播。不少人对其效果表示赞叹，并希望用它来「复活」逝者。

    但也有人担心，这些技术的持续进化会让网络视频变得更加真假难辨，或者被不法分子用于诈骗。看来，反诈手段要继续升级了。

    **GAIA 有什么创新点？**

    会说话的虚拟人物生成旨在根据语音合成自然视频，生成的嘴型、表情和头部姿势应与语音内容一致。以往的研究通过实施特定虚拟人物训练（即为每个虚拟人物训练或调整特定模型），或在推理过程中利用模板视频实现了高质量的结果。最近，人们致力于设计和改进零样本会说话的虚拟人物的生成方法（即仅有一张目标虚拟人物的肖像图片可以用于外貌参考）。不过，这些方法通过采用基于 warping 的运动表示、3D Morphable Model（3DMM）等领域先验来降低任务难度。这些启发式方法虽然有效，但却阻碍了从数据分布中直接学习，并可能导致不自然的结果和有限的多样性。

    本文中，来自微软的研究者提出了 GAIA（Generative AI for Avatar），其能够从语音和单张肖像图片合成自然的会说话的虚拟人物视频，在生成过程中消除了领域先验。

    

    GAIA 揭示了两个关键洞见：

    1. 用语音来驱动虚拟人物运动，而虚拟人物的背景和外貌（appearance）在整个视频中保持不变。受此启发，本文将每一帧的运动和外貌分开，其中外貌在帧之间共享，而运动对每一帧都是唯一的。为了根据语音预测运动，本文将运动序列编码为运动潜在序列，并使用以输入语音为条件的扩散模型来预测潜在序列；
    2. 当一个人在说出给定的内容时，表情和头部姿态存在巨大的多样性，这需要一个大规模和多样化的数据集。因此，该研究收集了一个高质量的能说话的虚拟人物数据集，该数据集由 16K 个不同年龄、性别、皮肤类型和说话风格的独特说话者组成，使生成结果自然且多样化。

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制-36氪](https://36kr.com/p/2543099800561414)

* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * 文字部分略，详见上面的网页链接

* [微软的这个项目厉害了... - @互联网的那点事的微博 - 微博](https://weibo.com/1627825392/Nv8Ya23gw?type=repost)

  * 互联网的那点事

    23-12-1 19:51
    发布于 安徽
    来自 微博视频号
    文字部分略，详见上面的网页链接 https://weibo.com/tv/show/1034:4974225992384581?from=old_pc_videoshow

* [Xu Tan on X: "🔥GAIA (Generative AI for Avatar) generates high-quality, natural, and spontaneous avatars given a single reference image driven by text/speech/video. ‼️ Some keywords of GAIA: Data/Model Scaling, ID/Motion Disentanglement, and Zero-Shot. 🔗Project page: https://t.co/qQUXlaAPmU" / X](https://twitter.com/xutan_tx/status/1731007471484027036)
  * GAIA (Generative AI for Avatar)（头像生成式人工智能）通过文本/语音/视频驱动，在单一参考图像的基础上生成高质量、自然、自发的头像。
    GAIA 的一些关键词 数据/模型缩放、ID/运动离散和零镜头。

* [Dreaming Tulpa 🥓👑 on X: "High quality AI generated talking heads are coming! GAIA can generate talking avatars from a single portrait image and speech clip. It even supports text prompts like \`sad\`, \`open mouth\` or \`surprise\` to guide video generation. Crazy times ahead 🤯 https://t.co/20WZOLMypz https://t.co/kgYLyzE1RJ" / X](https://twitter.com/dreamingtulpa/status/1730514359317590234)

  * 高品质人工智能生成的头像来了！

    GAIA 可以从单个肖像图像和语音片段生成会说话的化身。它甚至支持“悲伤”、“张开嘴”或“惊讶”等文字提示来指导视频生成。疯狂的时代即将来临



### 8、GitHub - arpitbansal297/Universal-Guided-Diffusion

Paper（论文地址）: 2023 [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121v1)

GitHub（代码地址）：https://github.com/arpitbansal297/Universal-Guided-Diffusion

Newsletter Post Title：AI in the workplace

Newsletter Post URL：

* [AI in the workplace](https://bensbites.beehiiv.com/p/ai-workplace)
  * Multimodal universal guidance for diffusion models without retraining.

Date：2023年6月8日



Summary：

This is a GitHub repository for Universal Guided Diffusion, which is a machine learning model for image and video generation. 

这是通用引导扩散的 GitHub 存储库，通用引导扩散是一种用于图像和视频生成的机器学习模型。



Tags：

machine learning, image generation, video generation, GitHub

机器学习、图像生成、视频生成、GitHub



* [Universal-Guided-Diffusion/README.md at main · arpitbansal297/Universal-Guided-Diffusion](https://github.com/arpitbansal297/Universal-Guided-Diffusion/blob/main/README.md)
  * The official PyTorch implementation of [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121). This repository has python implementation of universal guidance algorithm that enables controlling diffusion models by arbitrary guidance modalities without the need to retrain any use-specific components. Different guidance modalities we demonstrate are Human Identity, Segmentation Maps, Object Location, Image Style and Clip. Our implementation is based on the text-to-img model from [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and Imagenet Diffusion Model from [OpenAI's guided diffusion](https://github.com/openai/guided-diffusion).
    扩散模型通用指南的官方 PyTorch 实现。该存储库具有通用制导算法的 python 实现，可以通过任意制导方式控制扩散模型，而无需重新训练任何特定用途的组件。我们展示的不同引导模式包括人类身份、分割图、对象位置、图像样式和剪辑。我们的实现基于来自稳定扩散的文本到图像模型和来自 OpenAI 引导扩散的 Imagenet 扩散模型。

* [Universal Guidance for Diffusion Models,arXiv - CS - Machine Learning - X-MOL](https://newsletter.x-mol.com/paper/1626022089827893248?adv)

  * **扩散模型的通用指南**

    典型的扩散模型经过训练可以接受特定形式的调节，最常见的是文本，并且不能在没有重新训练的情况下以其他形式为条件。在这项工作中，我们提出了一种通用的指导算法，使扩散模型能够由任意指导方式控制，而无需重新训练任何特定于使用的组件。我们展示了我们的算法成功地生成了具有引导功能的高质量图像，包括分割、人脸识别、对象检测和分类器信号。代码可在 https://github.com/arpitbansal297/Universal-Guided-Diffusion 获得。

* [扩散模型的通用指导手册\_Zilliz\_InfoQ写作社区](https://xie.infoq.cn/article/c3c7dfb1947a98fddb858a36d)

  * 文字部分略，详见上面的网页链接

    

    

### 9、SceneScape: Text-Driven Consistent Scene Generation

项目地址：https://scenescape.github.io/

Paper（论文地址）: 2023 [SceneScape: Text-Driven Consistent Scene Generation](https://arxiv.org/abs/2302.01133)

Supplementary Material：https://scenescape.github.io/sm/index.html

GitHub（代码地址）：https://github.com/RafailFridman/SceneScape

Date：2023年2月3日



Summary：

The paper presents a method for text-driven perpetual view generation, which synthesizes long-term videos of various scenes solely based on an input text prompt. The method combines the generative power of a pre-trained text-to-image model with the geometric priors learned by a pre-trained monocular depth prediction model to achieve 3D consistency. The depth maps are used to construct a unified mesh representation of the scene, which is progressively constructed along the video generation process. The method generates diverse scenes, such as walkthroughs in spaceships, caves, or ice castles. 

本文提出了一种文本驱动的永久视图生成方法，该方法仅根据输入的文本提示合成各种场景的长期视频。该方法将预训练的文本到图像模型的生成能力与预训练的单目深度预测模型学习的几何先验相结合，以实现 3D 一致性。深度图用于构建场景的统一网格表示，该表示是沿着视频生成过程逐步构建的。该方法生成不同的场景，例如宇宙飞船、洞穴或冰堡中的演练。

How It Works ：

> ```
> We represent the generated scene with a unified mesh , which is constructed in an online fashion. Given a camera at Ct+1, at each synthesis step, a new frame is generated by projecting t into Ct+1, and synthesizing the newly revealed content by using a pre-trained text-to-image diffusion model. To estimate the geometry of the new synthesized content, we leverage a pre-trained depth prediction model; to ensure the predicted depth is consistent with the existing scene t, we deploy a test-time training, encouraging the predicted depth by the model to match the projected depth from t. We then update our mesh representation to form t+1 which includes the new scene content.
>  
> 我们用统一的网格  表示生成的场景，该网格以在线方式构建。给定 Ct+1 处的相机，在每个合成步骤中，通过将 t 投影到 Ct+1 中生成一个新帧，并使用 pre 合成新显示的内容-训练有素的文本到图像扩散模型。为了估计新合成内容的几何形状，我们利用预先训练的深度预测模型；为了确保预测深度与现有场景 t 一致，我们部署了测试时训练，鼓励模型的预测深度与 t 的预测深度相匹配。然后，我们更新网格表示以形成 t+1 ，其中包括新的场景内容。
>  
>  
>  A visualization of the resulting meshes, produced by our method and post processed with Poisson surface reconstruction.
> 结果网格的可视化，由我们的方法生成并通过泊松曲面重建进行后处理。
> ```



Tags：

text-driven, perpetual view generation, 3D consistency, mesh representation, video generation

文本驱动、永久视图生成、3D 一致性、网格表示、视频生成





### 10、Scalable Adaptive Computation for Iterative Generation

Paper（论文地址）: 2022 [ Scalable Adaptive Computation for Iterative Generation](https://arxiv.org/abs/2212.11972)



Date：2022年12月23日

Summary：

The Recurrent Interface Network (RIN) is a neural net architecture that allocates computation adaptively to the input according to the distribution of information, allowing it to scale to iterative generation of high-dimensional data. RINs yield state-of-the-art image and video generation without cascades or guidance, while being domain-agnostic and up to 10x more efficient compared to specialized 2D and 3D U-Nets.

循环接口网络（RIN）是一种神经网络架构，它根据信息的分布自适应地将计算分配给输入，从而使其能够扩展到高维数据的迭代生成。 RIN 无需级联或引导即可生成最先进的图像和视频，同时与领域无关，并且与专门的 2D 和 3D U-Net 相比，效率提高了 10 倍。

Tags：

neural networks, iterative generation, image generation, video generation

神经网络、迭代生成、图像生成、视频生成



网友个人博客的阅读笔记：

* [Scalable Adaptive Computation for Iterative Generation | Qiang Zhang](https://zhangtemplar.github.io/iterative-generation/)
  * 这里的主要创新是将输入标记映射到更短的潜在变量。潜伏可以从先前的迭代（扩散过程）中初始化。因此，新方法可以实现与常规扩散方法相似的视觉保真度，但成本仅为常规扩散方法的 1/10。



Twitter 推特 社交媒体资讯：

* [AK on X: "Scalable Adaptive Computation for Iterative Generation abs: https://t.co/1HOY8H1UBe RIN, a NN architecture that allocates computation adaptively to the input according to the distribution of information, allowing it to scale to iterative generation of high-dimensional data https://t.co/FW5TfIbayy" / X](https://twitter.com/_akhaliq/status/1606104485390065664)

  * 迭代生成的可扩展自适应计算

    RIN，一种NN体系结构，根据信息的分布将计算自适应地分配到输入，从而使其扩展到迭代生成高维数据





### 11、Keras documentation: Denoising Diffusion Probabilistic Model

URL：https://keras.io/examples/generative/ddpm/

#### Introduction 介绍

Generative modeling experienced tremendous growth in the last five years. Models like VAEs, GANs, and flow-based models proved to be a great success in generating high-quality content, especially images. Diffusion models are a new type of generative model that has proven to be better than previous approaches.
生成建模在过去五年中经历了巨大的增长。事实证明，VAE、GAN 和基于流的模型等模型在生成高质量内容（尤其是图像）方面取得了巨大成功。扩散模型是一种新型的生成模型，已被证明比以前的方法更好。

We implement the Denoising Diffusion Probabilistic Models paper or DDPMs for short in this code example. It was the first paper demonstrating the use of diffusion models for generating high-quality images. The authors proved that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling that generates the best quality results.
我们在此代码示例中实现了去噪扩散概率模型论文（简称 DDPM）。这是第一篇演示如何使用扩散模型生成高质量图像的论文。作者证明，扩散模型的某些参数化揭示了训练期间多个噪声级别的去噪分数匹配以及采样期间退火的朗之万动力学的等价性，从而产生最佳质量的结果。

#### Conclusion 结论

We successfully implemented and trained a diffusion model exactly in the same fashion as implemented by the authors of the DDPMs paper. You can find the original implementation here.
我们成功地实现并训练了一个扩散模型，其方式与 DDPM 论文作者所采用的方式完全相同。您可以在这里找到原始实现。

The original implementation （原始实现）：https://github.com/hojonathanho/diffusion

GitHub（源代码地址）：https://github.com/keras-team/keras-io/blob/master/examples/generative/ddpm.py

View in Colab（在 Colab 中查看）：https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/ddpm.ipynb

Date：2022年12月13日

Summary：

This URL link  above is to the Keras documentation for the Denoising Diffusion Probabilistic Model (DDPM), which is a generative model that can be used for image and video generation. The page provides an overview of the model and its architecture, as well as code examples for training and using the model.

上面的 URL 链接是去噪扩散概率模型 (DDPM) 的 Keras 文档，DDPM 是一种可用于图像和视频生成的生成模型。该页面提供了模型及其架构的概述，以及用于训练和使用模型的代码示例。

Tags：

Keras, generative model, DDPM, image generation, video generation

Keras、生成模型、DDPM、图像生成、视频生成





### 12、Elai.io - AI Video Generation Platform

官方网站URL：https://elai.io/

温馨提示：在官方网站注册后，可以免费生成1分钟的视频。

Date：2022年12月5日

Summary：

Elai.io is an automated AI video generation platform that allows users to create personalized videos using digital avatars from text. The platform eliminates the need for a camera, studio, and greenscreen, making video creation quick and easy.

Elai.io 是一个自动化人工智能视频生成平台，允许用户使用文本中的数字化身创建个性化视频。该平台消除了对摄像机、工作室和绿幕的需求，使视频创建变得快速、轻松。

Features（主要功能特点）:

Multi-lingual voice cloning 多语言语音克隆：将您自己的声音克隆成 28 种语言，并利用它将您的所有想法变为现实。只需输入语音文本，头像就会用您的声音进行叙述。

Automated translations 自动翻译：一键式视频翻译将帮助您吸引全球观众。将您的视频翻译成 75 种可用语言。

AI Storyboard 人工智能故事板：适合学习和发展专业人士的完美内容创建工具。只需点击几下即可制作脚本，创建课程大纲，然后将其转变为引人入胜的视频。

Article-to-Video converter 文章到视频转换器：将您的博客文章从 URL 转换为带旁白的视频。将您博客文章的链接粘贴到亦来在线视频生成器中，让我们的平台完成剩下的工作。

PPTX-to-Video PPTX 转视频：上传 PPTX 文件，它将转换为视频幻灯片，创建具有可编辑组件的完全交互式演示文稿。

Avatar Dialogs 头像对话框：在幻灯片中添加 2 个头像以创建基于场景的学习视频。

Personalization at Scale 大规模个性化：大规模创建个性化视频营销活动，传递更具针对性的信息并与受众建立更牢固的关系。

Create your avatar 创建你的头像：创建您公司代表的数字化身，并使用 Elai 克隆他们的声音。

Screen Recorder 屏幕录像机：Elai.io 的屏幕录制功能允许用户轻松捕获并上传屏幕录制内容，以便在视频演示中使用它们。

等



Tags：

AI, video generation, digital avatars

人工智能、视频生成、数字化身



媒体评价：

* [Central Eastern Europe is a hub for exciting startups | WIRED UK](https://www.wired.co.uk/bc/article/google-for-startups-eastern-europe)
  * 利用先进的人工智能，Elai.io 的软件即服务工具允许用户仅使用文本轻松创建视频，将单词放入数字化身的嘴中。它提供了一个模板和动画库，支持超过 65 种语言，并且头像基于各种现实生活中的演员。

* [Elai.io - Product Information, Latest Updates, and Reviews 2023 | Product Hunt](https://www.producthunt.com/products/elai-io)

  * Elai.io 帮助您使用人工智能生成的人类从纯文本创建教育和营销视频内容。

* Forbes：2021年，初创公司Elai.io开发了一款将文本转换为视频内容的产品。作为输出，用户会收到由人工智能化身显示和发声的演示文稿。

  



### 13、Introducing a Practical Tool to Generate Images and Videos with AI

URL：https://twitter.com/tall/status/1587913529587249152

工具地址：https://stableboost.ai/

Date：2022年11月3日

Summary：

Tal Stramer has announced a new project, https://t.co/4hhV3KfNZi , which is a practical tool to generate images and videos with AI. The project was advised by Andrej Karpathy.

Tal Stramer 宣布了一个新项目https://t.co/4hhV3KfNZi ，这是一个利用 AI 生成图像和视频的实用工具。该项目由安德烈·卡帕蒂 (Andrej Karpathy) 提供建议。

> 以下援引自上述URL中项目作者Tal Stramer发言
>
> Tal Stramer：Why another generative AI service? To create the perfect image with AI, you often need to generate hundreds of variations, but existing tools only let you generate a few images at a time. Stableboost is built from the ground up to let you quickly generate a lot of images.
> 为什么需要另一种生成式人工智能服务？要使用 AI 创建完美的图像，您通常需要生成数百种变体，但现有工具只能让您一次生成几张图像。 Stableboost 是从头开始构建的，可让您快速生成大量图像。
>
> After generating many images for a prompt, Stableboost lets you interactively discover your favorites. It learns your preferences based on selecting a few images you like and recommends you similar images.
> 在为提示生成许多图像后，Stableboost 可以让您以交互方式发现您最喜欢的图像。它通过选择您喜欢的一些图像来了解您的偏好，并向您推荐类似的图像。
>
> Crafting a prompt to generate the images you want can involve a lot of trial and error. Stableboost speeds this up by letting you try out different variations of your prompt all at once.
> 制作提示来生成您想要的图像可能需要大量的试验和错误。 Stableboost 通过让您同时尝试不同的提示变体来加快速度。
>
> For fun, Stableboost also lets you generate hypnotic video animations that smoothly interpolate between text prompts.
> 为了好玩，Stableboost 还可以让您生成催眠视频动画，在文本提示之间平滑插入。
>
> Stableboost also supports many of the standard features people have come to expect from these types of services: Image-to-image, inpainting, face restoration, upscaling, etc.
> Stableboost 还支持人们期望从这些类型的服务中获得的许多标准功能：图像到图像、修复、面部恢复、升级等。
>
> Try Stableboost for free at [https://stableboost.ai](https://t.co/zWIG6ZwxsI) . You get 500 free credits every month and its 1 cent per image after that (the price is minimal so I can cover my costs). Enjoy!
> 访问 [https://stableboost.ai](https://t.co/zWIG6ZwxsI) 免费试用 Stableboost。您每月可以获得 500 个免费积分，之后每张图片 1 美分（价格很低，所以我可以支付我的费用）。享受！
>
> yes, you can upload an image to use as a reference and  set the style you want as the prompt. play with the guidance scale and initial image similarity settings to get good results. like this
> 是的，您可以上传图像作为参考，并设置您想要的样式作为提示。使用引导比例和初始图像相似度设置以获得良好的结果。像这样
>
> ![example](https://gitee.com/junhaoyu/work20221111/raw/master/img/202312191351540.jpg)

Tags：

AI, image generation, video generation, machine learning

人工智能、图像生成、视频生成、机器学习





### 14、Phenaki: A Model for Generating Videos from Text

URL：https://twitter.com/doomie/status/1577713150530445312

5/ phanaki最值得注意的功能是它具有随着时间的流逝而变化的提示来生成长视频的能力。这些可以将其视为故事，用户讲述并创建动态变化的场景。

Paper（论文地址）: 2022 [Phenaki: Variable Length Video Generation From Open Domain Textual Description](https://arxiv.org/abs/2210.02399)

https://pub-bede3007802c4858abc6f742f405d4ef.r2.dev/paper.pdf

我们提出 Phenaki，这是一种能够在给定一系列文本提示的情况下进行逼真视频合成的模型。由于计算成本、高质量文本视频数据数量有限以及视频长度可变，从文本生成视频尤其具有挑战性。为了解决这些问题，我们引入了一种新的因果模型来学习视频表示，该模型将视频压缩为离散标记的小型表示。该分词器及时使用因果注意力，这使得它可以处理可变长度的视频。为了从文本生成视频标记，我们使用以预先计算的文本标记为条件的双向屏蔽转换器。生成的视频令牌随后被去令牌化以创建实际视频。为了解决数据问题，我们演示了对大量图像文本对以及少量视频文本示例进行联合训练如何产生超出视频数据集中可用的泛化能力。与上一代视频相比 方法，Phenaki 可以在开放域中根据一系列提示（即时间可变文本或故事）生成任意长视频。据我们所知，这是第一次有论文研究根据时间变量提示生成视频。此外，所提出的视频编码器-解码器在时空质量和每个视频的标记数量方面优于目前文献中使用的所有每帧基线。 

项目地址：https://phenaki.github.io/

Date：2022年10月10日

Summary：

Dumitru Erhan introduces Phenaki, a model for generating videos from text with prompts that can change over time. The model is able to generate videos that can be as long as multiple minutes.

Dumitru Erhan 推出了 Phenaki，这是一种从文本生成视频的模型，其提示可以随时间变化。该模型能够生成长达数分钟的视频。

Tags：

AI, video generation, natural language processing

人工智能、视频生成、自然语言处理





### 15、Imagen Video: A New Text-Conditioned Video Diffusion Model

Imagen Video：A New Text-Conditioned Video Diffusion Model 一种新的文本条件视频扩散模型

URL：https://twitter.com/hojonathanho/status/1577712621037445121

兴奋地宣布Imagen Video，这是我们新的文本条件视频扩散模型，该模型生成1280x768 24fps HD视频！#ImagenVideo

 Imagen Video 的独特功能：例如生成不同艺术风格的视频、3D 理解以及文本渲染和动画。查看论文了解更多详细信息

借助渐进式蒸馏，Imagen Video 可以为每个子模型仅使用 8 个扩散步骤生成高质量视频。这大大加快了视频生成时间，大约提高了 18 倍。

另外，请查看 Phenaki，这是 Google Research 的一种用于文本到视频的补充方法，可以为一系列文本提示生成长而连贯的视频： https://phenaki.github.io 
我们期待结合 Phenaki 和 Imagen Video 的优势！

Paper（论文地址）: 2022 [Imagen Video: High Definition Video Generation with Diffusion Models](https://arxiv.org/abs/2210.02303)

https://imagen.research.google/video/paper.pdf

Date：2022年10月10日

Summary：

Jonathan Ho announces Imagen Video, a new text-conditioned video diffusion model that generates high-quality videos.

Jonathan Ho 宣布推出 Imagen Video，这是一种新的文本调节视频传播模型，可生成高质量视频。

Tags：

AI, video generation, deep learning

人工智能、视频生成、深度学习



媒体资讯：

* [Stable Diffusion、DreamFusion、Make-A-Video、Imagen Video 和下一步\_AI\_Luhui Hu\_InfoQ精选文章](https://www.infoq.cn/article/bkptwsyeidednaobvekv)

  * Google 的 Imagen Video 是一个基于视频扩散模型级联的文本条件视频生成系统。

    给定一个文本提示，Imagen Video 使用基本视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。

    它由七个子模型组成，分别执行文本条件视频生成、空间超分辨率和时间超分辨率。整个级联生成 1280×768（宽×高）的高清视频，每秒 24 帧，持续 128 帧 （~5.3 秒），大约 1.26 亿像素。

    Imagen Video 示例：“一束秋天的树叶落在平静的湖面上，形成文本‘Imagen Vide’。平滑。”生成的视频分辨率为 1280×768，持续时间为 5.3 秒，每秒 24 帧（来源：Imaged Video）

    原文链接：https://towardsdatascience.com/generative-ai-878909fb7868





### 16、李飞飞携斯坦福联袂谷歌，推出了用于生成逼真视频的扩散模型 W.A.L.T

2023 年 12 月 12 日，李飞飞携斯坦福联袂谷歌，用 Transformer 生成了逼真视频，效果媲美 Gen-2 比肩 Pika，正式推出了用于生成逼真视频的扩散模型 W.A.L.T。2023 年俨然已成 AI 视频元年。

Paper（论文）：2023  [Photorealistic Video Generation with Diffusion Models](https://arxiv.org/abs/2312.06662)

https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf  This is a Transformer-based diffusion model trained on image and video generation in a shared latent space. 这是一个在共享潜在空间中训练图像和视频生成的，基于 Transformer 的扩散模型。



Twitter 推特 社交媒体资讯：

* [Fei-Fei Li on X: "I’m very excited by this new, transformer-based diffusion model for video generation led by my student @agrimgupta92 , in collaboration with amazing @GoogleAI researchers!🤩👇" / X](https://twitter.com/drfeifei/status/1734266529431044152)



媒体资讯：

* [AI 生成视频，且看 Pika&W.A.L.T | 北拓研究 - Foresight News](https://foresightnews.pro/article/detail/50094)

  * W.A.L.T 的关键，是将图像和视频编码到一个共享的潜在空间中。Transformer 主干通过具有两层窗口限制注意力的块来处理这些潜在空间——空间层捕捉图像和视频中的空间关系，而时空层模拟视频中的时间动态，并通过身份注意力掩码传递图像。而文本调节，是通过空间交叉注意完成的。

    W.A.L.T 解决视频生成建模难题

  * 英伟达高级科学家 Jim Fan 转发评论道：2022 年是影像之年，2023 是声波之年，而 2024，是视频之年。

  * **03**

    **结语**

    Pika 的闯入和 W.A.L.T 的推出，无疑再次搅动了 AI 视频领域的格局，战场上已经硝烟滚滚。

    AI 视频的未来发展趋势涉及多个方面，包括技术进步、应用场景扩展以及市场竞争格局等：

    - 更高级的视频生成和编辑技术：随着深度学习和生成模型的不断发展，预计会有更先进、更逼真的视频生成技术。这可能包括更精细的图像合成、更智能的视频编辑和更自然的语音合成。
    - 增强现实（AR）和虚拟现实（VR）应用：AI 视频将在 AR 和 VR 领域发挥关键作用，为用户提供更沉浸式的体验。这可能包括虚拟现实培训、虚拟旅游和虚拟会议等应用。
    - 个性化内容生成：AI 视频有望推动个性化内容的生成，根据用户的兴趣和偏好，为他们定制特定内容。这可能包括个性化广告、电影和在线教育等领域。
    - 实时视频处理：随着计算能力的提高，未来 AI 视频系统可能更加注重实时处理，以满足对实时反馈和互动性的需求。这对于在线直播、视频会议和实时事件报道等场景具有重要意义。
    - 自动化和协作：AI 视频在制作、编辑和发布过程中的自动化将继续增加。多个 AI 系统可能协同工作，共同完成视频内容的创作和优化。

    AI 视频未来的竞争格局可能涉及到技术领导者、创新型初创公司以及传统的媒体和技术巨头。争夺关键技术专利、拥有大规模训练数据、拓展应用场景和提供稳定性可靠的解决方案都将是竞争的关键因素。在不同的市场细分领域，可能会涌现出一系列专业化的解决方案。

    需要注意的是，AI 技术的发展伴随着伦理和隐私等问题，因此相关政策和规范的建立也将对竞争格局产生影响。

    算力将会是影响 AI 视频服务的一个重要因素，特别是在 C 端（面向消费者）服务中。以下则是与算力相关的影响因素：

    - 实时性和互动性：高算力可以支持更快的实时处理和更高的互动性。在视频通话、实时直播以及虚拟和增强现实应用中，快速的算力可以确保流畅的体验。
    - 高质量的生成和处理：高算力对于生成高质量、高分辨率的视频内容至关重要。这对于视频编辑、特效添加以及其他创意性的应用非常重要。
    - 大规模数据处理：针对大规模数据的训练和处理需要强大的算力。这对于训练复杂的深度学习模型、提高视频识别和分析的准确性以及支持大规模的用户请求都至关重要。
    - 云服务的影响：对于 C 端用户而言，云服务中的高算力可以提供更强大的视频处理和存储能力。这使得用户可以享受到云端处理的便利性，而无需担心本地设备的性能限制。
    - 成本和可扩展性：算力的成本和可扩展性也是重要考虑因素。成本高昂的算力可能导致服务费用的增加，而可扩展性差则可能限制了服务的用户规模。

    总体而言，随着技术的不断进步，算力在 AI 视频服务中的重要性将继续增加。创新性的解决方案和对算力的有效利用将有助于提高服务的竞争力，并为用户提供更出色的体验。

    相信随着未来算力的进一步提升，AI 视频领域的推陈出新一定更加令人期待。 





### 17、HeyGen

由 Joshua Xu 和 Wayne Liang 于 2020 年 XNUMX 月在洛杉矶创立， HeyGen（最初为“Movio”）是一个独特的视频平台，它使用生成式人工智能在几分钟内创建令人惊叹的专业视频。

借助其文本转语音功能，您可以将脚本转换为 300 多种语言、40 多种语音的自然发音单词。 然后，您可以从 100 多个代表不同种族、年龄和姿势的人工智能头像中进行选择，并通过自然的口型同步功能逐行背诵您的脚本。

因此，这可以为企业节省大量时间和金钱，而不是雇用演员和购买昂贵的录音设备。 HeyGen 非常适合想要在预算内创建视频内容的企业，无论是创建引人入胜的产品视频还是培训视频。

官网地址：https://www.heygen.com/

Founders Introduction 创始人介绍：https://www.heygen.com/about-us

产品demo：[HeyGen - AI Video Generator](https://demo.heygen.com/free-video)

产品特点：HeyGen是一个AI视频创建平台，帮助你用生成性人工智能创建引人入胜的商业视频，就像为各种使用案例制作PowerPoint幻灯片一样容易，该工具免费版提供1分钟的时长。

HeyGen 的特点是它可以使用 AI 头像来作为视频的演讲者，你可以从它的库中选择或者上传你自己的面部和声音来创建个性化的视频。

官网博客：[All in One AI Video Generator - HeyGen.com | HeyGen Blog](https://www.heygen.com/article/ai-video-generator)

* [HeyGen AI: Free AI Video Generator Pricing, Features, and Alternatives - Cloudbooklet AI](https://www.cloudbooklet.com/heygen-ai-video-generator-free/)



官方社交媒体账号： [heygen - Link in Bio & Creator Tools | Beacons](https://beacons.ai/HeyGen)

LinkedIn：[HeyGen: 简介 | LinkedIn](https://www.linkedin.com/company/heygen/?viewAsMember=true)

Tiktok：https://www.tiktok.com/@heygenofficial

Youtube：[HeyGen - YouTube](https://www.youtube.com/@heygen_official)

Twitter（X）：[HeyGen (@HeyGen\_Official) / X](https://twitter.com/heygen_official)

Facebook： [HeyGen User Group 👨🏻‍💻 | Facebook](https://www.facebook.com/groups/345553694144336)

Discord：[(Discord | #🚀︱start | HeyGen](https://discord.com/channels/963360360885264384/1051999877837631529)



媒体资讯：

* [HeyGen Review: The Best AI Video Generator for Businesses?](https://www.unite.ai/heygen-review/#:~:text=HeyGen%20Review%3A%20Final%20Thoughts&text=HeyGen%20has%20significantly%20improved%20my,wide%20range%20of%20customization%20options.)

* [HeyGen 评论：最适合企业的人工智能视频生成器？](https://www.unite.ai/zh-CN/%E6%B5%B7%E6%A0%B9%E8%AF%84%E8%AE%BA/)

  * 在当今快节奏的世界中，每个人都在寻找更有效地创建内容的方法。 但你听说过吗 [人工智能视频生成器](https://www.unite.ai/zh-CN/最好的人工智能视频生成器/)?

    HeyGen 是其中之一。 它是一款人工智能驱动的视频生成器，只需点击几下即可帮助企业创建具有专业外观的视频。

* [HeyGen AI: Free AI Video Generator Pricing, Features, and Alternatives - Cloudbooklet AI --- HeyGen AI：免费 AI 视频生成器定价、功能和替代方案 - Cloudbooklet AI](https://www.cloudbooklet.com/heygen-ai-video-generator-free/)

* [HeyGen：AI创业，我们如何在7个月内达到100万美元收入｜Z Circle](https://mp.weixin.qq.com/s?__biz=MjM5NDk5MTA0MQ==&mid=2652318720&idx=1&sn=7cba20ec976fe46b0804cd0e6c7e8e5e&sharer_shareinfo=dd0e6f646cdf6d515f9039f37ad27ef4&sharer_shareinfo_first=cb10f561290435b6b00fd4829bd0c9ac#rd)





### 18、Pictory

官网地址：https://pictory.ai/

产品特点：https://pictory.ai/all-features

官网博客：https://pictory.ai/blog

官方社交媒体账号：

LinkedIn： [ Pictory: 简介 | LinkedIn](https://www.linkedin.com/company/pictory/)

Youtube： [Pictory - YouTube](https://www.youtube.com/pictory)

Twitter（X）：[pictory (@pictoryai) / X](https://twitter.com/pictoryai)

Facebook：* [Pictory Facebook](https://www.facebook.com/profile.php?id=100083315262677)

媒体资讯：

* [10 "Best" AI Video Generators (December 2023) - Unite.AI](https://www.unite.ai/best-ai-video-generators/)

* [10 个“最佳”AI 视频生成器（2023 年 XNUMX 月） - Unite.AI](https://www.unite.ai/zh-CN/%E6%9C%80%E5%A5%BD%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E5%99%A8/)

  * Pictory是一款AI视频生成器，可让您轻松创建和编辑高质量视频。 该工具最好的方面之一是您不需要任何视频编辑或设计经验。 

    首先，您提供一个脚本或文章，它将作为您的视频内容的基础。 例如，Pictory 可以将您的博客文章变成引人入胜的视频，用于社交媒体或您的网站。 对于希望提高参与度和质量的个人博主和公司来说，这是一个很棒的功能。 由于它基于云，因此可以在任何计算机上运行。 

    Pictory 还允许您使用文本轻松编辑视频，这非常适合编辑网络研讨会、播客、Zoom 录音等。 它使用简单，只需几分钟即可提供专业的结果，帮助您扩大受众群体并建立您的品牌。 

    Pictory 的另一个强大功能是您可以创建可共享的视频精彩片段，这对于那些想要创建预告片或在社交媒体上共享短片的人来说非常有用。 除了这些强大的功能之外，您还可以自动为视频添加字幕并自动总结长视频。 

    以下是 Pictory 的一些主要功能： 

    - 基于文章或脚本的视频
    - 使用文本编辑视频
    - 创建可共享的视频精彩片段
    - 自动为视频添加字幕和摘要

* [Pictory Review (December 2023): The Best AI Video Generator? - Unite.AI](https://www.unite.ai/pictory-review/)

* [Pictory Review（2023 年 XNUMX 月）：最好的 AI 视频生成器？ - 联合人工智能](https://www.unite.ai/zh-CN/%E5%9B%BE%E7%89%87%E8%AF%84%E8%AE%BA/)
  * 简而言之： Pictory是轻松视频营销的终极解决方案！








## AI Funding Rounds

### 1、SeiSei.ai

SeiSei.ai is a text-to-video generation platform. SeiSei's algorithms can automatically change video lip sync and language, creating seamless and realistic videos.
SeiSei.ai 是一个文本到视频生成平台。 SeiSei 的算法可以自动更改视频口型同步和语言，创建无缝且逼真的视频。

Industry（所属行业）：Artificial Intelligence, SaaS, Video 人工智能、SaaS、视频

Company Description（公司描述）：SeiSei.ai is a text-to-video generation platform. SeiSei.ai 是一个文本到视频生成平台。

Website（官方网站）：https://www.seisei.ai/

Announced Date（公布融资消息的日期）：2023年8月2日

Funding Round（融资轮次）：Seed（种子轮）

Number of Funding Rounds（融资轮数）：1

Funding amount：undisclosed （融资金额未公开）

Crunchbase（CB数据库）：[SeiSei.ai - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/rgen-ai)

Ethics Guidlines （道德准则）：https://www.seisei.ai/ethics

Official social media account（官方社交媒体账号）：

LinkedIn: https://www.linkedin.com/company/seisei-ai

YouTube:  [Demo - YouTube](https://www.youtube.com/playlist?list=PLqKwFgGmv-tN-ldy5zxNceST-_q_8UQF0)

Instagram： [SeiSei.ai (@seisei.ai\_official) · Instagram 照片和视频](https://www.instagram.com/seisei.ai_official/)

Facebook：https://www.facebook.com/seiseiai



来自Crunchbase（CB数据库）：

Who are [SeiSei.ai](https://www.crunchbase.com/organization/rgen-ai)'s competitors? Alternatives and possible competitors to [SeiSei.ai](https://www.crunchbase.com/organization/rgen-ai) may include [Guru](https://www.crunchbase.com/organization/guru-technologies), [ZeroEyes](https://www.crunchbase.com/organization/zeroeyes), and [Imaginario AI](https://www.crunchbase.com/organization/imaginario)
SeiSei.ai 的竞争对手是谁？ SeiSei.ai 的替代品和可能的竞争对手可能包括 Guru、ZeroEyes 和 Imaginario AI.



Democratizing Generating Personalized Videos at scale.

大规模生成个性化视频。

使用步骤：

1、Upload 上传 Add Your Video and Audio Material 添加您的视频和音频材料

2、Input Your Script 输入您的脚本 Craft Your Message with Custom Variables 使用自定义变量制作您的消息

3、Generate 产生 Watch Our AI Craft Your Personalized Videos 观看我们的人工智能制作您的个性化视频

About（关于）：

##### Our mission 我们的使命

##### Make video campaigns easy for anyone 让任何人都能轻松开展视频营销活动

When individuals unite in pursuit of a shared vision, fueled by passion, the collective effort surpasses the sum of its parts. At SeiSei.ai, we hold a deep appreciation for our community, which includes both our dedicated team and our valued clients. Our culture is rooted in the conviction that the strength of ideas should eclipse considerations of title or hierarchy. Transparency serves as the cornerstone of our cultural ethos.
当个人在激情的推动下团结起来追求共同的愿景时，集体的努力就会超越各个部分的总和。在 SeiSei.ai，我们对我们的社区深表感谢，其中包括我们敬业的团队和尊贵的客户。我们的文化植根于这样的信念：思想的力量应该超越头衔或等级制度的考虑。透明度是我们文化精神的基石。





### 2、VCAT.AI

VCAT.AI is a business-to-business (B2B) subscription software (SaaS) based on AI technology. Create mass marketing creatives as video and banner images just with an product URL or TEXT.

VCAT.AI是一款基于人工智能技术的企业对企业（B2B）订阅软件（SaaS）。只需使用产品 URL 或文本即可创建视频和横幅图像等大众营销创意。

Industry（所属行业）：Advertising, Artificial Intelligence, Creative Agency, SaaS, Video Editing 广告、人工智能、创意机构、SaaS、视频编辑

Company Description（公司描述）：AI-based video generation SaaS solution. 基于AI的视频生成SaaS解决方案。

Website（官方网站）：https://vcat.ai/

Location：Seoul 韩国首尔

Announced Date（公布融资消息的日期）：2023年4月4日

Funding Round（融资轮次）：：Series A （A轮）

Number of Funding Rounds（融资轮数）：1

Funding Stage：Early Stage Venture 早期风险投资

Funding amount：$8,000,000    800万美元

Lead Investors：Premier Partners

Investor Names：KB Investment, KT investment, Premier Partners, SmileGate

Crunchbase（CB数据库）： [VCAT.AI - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/vcat-ai)

Official social media account（官方社交媒体账号）：

LinkedIn: https://www.linkedin.com/company/ai-vcat/

YouTube:  [VCAT AI - YouTube](https://www.youtube.com/channel/UC9HwVtVf72CTxiuunStO3Ag)



来自Crunchbase（CB数据库）：

Where is VCAT.AI's headquarters? VCAT.AI is located in Seoul, Seoul-t'ukpyolsi, South Korea
VCAT.AI的总部在哪里? VCAT.AI位于韩国Seoul-t'ukpyolsi首尔.

Who invested in VCAT.AI? VCAT.AI has 4 investors including KB Investment and Premier Partners
VCAT.AI是谁投资的？VCAT.AI有KB Investment、Premier Partners等4家投资方.

How much funding has VCAT.AI raised to date? VCAT.AI has raised $8M
VCAT.AI 迄今为止筹集了多少资金？VCAT.AI 已筹集 800 万美元.

When was the last funding round for VCAT.AI? VCAT.AI closed its last funding round on Apr 4, 2023 from a Series A round.
VCAT.AI 的最后一轮融资是什么时候？VCAT.AI 于 2023 年 4 月 4 日结束了 A 轮融资的最后一轮融资。

Who are VCAT.AI's competitors? Alternatives and possible competitors to VCAT.AI may include SandboxAQ, Sidecar, and Prevedere
VCAT.AI的竞争对手是谁？ VCAT.AI 的替代品和可能的竞争对手可能包括 SandboxAQ、Sidecar 和 Prevedere.



### 3、Boolean vector BOOLV

Boolean vector BOOLV is an AI video generation SaaS service provider. They provide SaaS services for e-commerce clients. They developed long-term and fruitful collaborations with numerous renowned firms in fashion, furniture brands, academia, and industry.
布尔向量BOOLV是一家AI视频生成SaaS服务商。他们为电子商务客户提供 SaaS 服务。他们与时尚界、家具品牌、学术界和工业界的众多知名公司建立了长期且富有成效的合作。

Industry（所属行业）：Artificial Intelligence 人工智能

Company Description（公司描述）：Boolean vector BOOLV is an AI video generation SaaS service provider. 布尔向量BOOLV是一家AI视频生成SaaS服务商。

Website（官方网站）：https://boolv.tech/

Announced Date（公布融资消息的日期）：2022年3月1日

Funding Round（融资轮次）：Series A （A轮）

Number of Funding Rounds（融资轮数）：1

Funding Stage：Early Stage Venture 早期风险投资

Funding amount：$10,000,000     1000万美元

Investor Names：Decent Capital, Kuehne + Nagel, Linear Capital, UpHonest Capital, Volcanics Venture

Crunchbase（CB数据库）： [Boolean vector BOOLV - Crunchbase Company Profile & Funding](https://www.crunchbase.com/organization/boolean-vector-boolv)

Official social media account（官方社交媒体账号）：

Twitter： [Boolv Tech (@boolvtech) / X](https://twitter.com/boolvtech)

YouTube:  [Boolvideo For Content Creator - YouTube](https://www.youtube.com/channel/UCSPT_tV0QfnGsC4KFSVDSkw)

Pinterest：[Pinterest](https://www.pinterest.com/boolv_tech/)

Instagram：[Boolv-Boolvideo&Booltool (@boolvtech) · Instagram 照片和视频](https://www.instagram.com/boolvtech/)

Medium： [BoolvTech – Medium --- 博尔夫科技 – Medium](https://medium.com/@boolv)

LinkedIn: https://www.linkedin.com/company/boolv-tech

Next generation of intelligent video platform that empowers brands.

为品牌赋能的下一代智能视频平台。

关于
BOOLV Tech provides SaaS software technology services for e-commerce clients, focusing on data and AI applications. Founded in 2021, we have completed 10m USD Angel round financing. Our headquarter and R&D center are established in Shenzhen Nanshan Free Trade Zone.

Our team consists of alumni from University of Pennsylvania, University of Oxford, University of Berkeley, The University of Hong Kong, etc. Core members are from leading tech companies and institutions such as Bytedance, Tencent and Data Science Institute (DSI) of Imperial College London, who are experts in ToB Saas, AI algorithm, and data analytics field.

BOOLV Tech为电商客户提供SaaS软件技术服务，专注于数据和人工智能应用。成立于2021年，已完成1000万美元天使轮融资。我们的总部和研发中心设立在深圳南山保税区。

我们的团队由来自宾夕法尼亚大学、牛津大学、伯克利大学、香港大学等的校友组成，核心成员来自字节跳动、腾讯、帝国理工数据科学研究所（DSI）等领先科技公司和机构London，ToB Saas、人工智能算法和数据分析领域的专家。

Facebook：https://www.facebook.com/profile.php?id=100083482763930



Product Hunt 链接：https://www.producthunt.com/products/boolvideo

来自Crunchbase（CB数据库）：

Where is [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv)'s headquarters?

[Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) is located in [Shenzhen](https://www.crunchbase.com/search/organizations/field/organizations/location_identifiers/shenzhen-guangdong),[ Guangdong](https://www.crunchbase.com/search/organizations/field/organizations/location_identifiers/guangdong-china),[ China](https://www.crunchbase.com/search/organizations/field/organizations/location_identifiers/china-500a)
布尔向量BOOLV的总部在哪里？布尔向量BOOLV位于中国广东深圳.

Who invested in [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv)? [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) has [5](https://www.crunchbase.com/search/principal.investors/field/organizations/num_investors/boolean-vector-boolv) investors including [UpHonest Capital](https://www.crunchbase.com/organization/uphonest-capital) and [Volcanics Venture](https://www.crunchbase.com/organization/volcanics-venture)
布尔向量BOOLV是谁投资的？布尔向量BOOLV有5个投资者，包括UpHonest Capital和Volcanics Venture.

How much funding has [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) raised to date? [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) has raised [$10M](https://www.crunchbase.com/search/funding_rounds/field/organizations/funding_total/boolean-vector-boolv)
布尔向量 BOOLV 迄今为止筹集了多少资金？布尔向量 BOOLV 已筹集 1000 万美元.

When was the last funding round for [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv)? [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) closed its last funding round on [Mar 1, 2022](https://www.crunchbase.com/search/funding_rounds/field/organizations/last_funding_at/boolean-vector-boolv) from a [Series A](https://www.crunchbase.com/search/funding_rounds/field/organizations/last_funding_type/boolean-vector-boolv) round.
布尔向量 BOOLV 的最后一轮融资是什么时候？布尔向量 BOOLV 于 2022 年 3 月 1 日结束了 A 轮融资的最后一轮融资。

Who are [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv)'s competitors? Alternatives and possible competitors to [Boolean vector BOOLV](https://www.crunchbase.com/organization/boolean-vector-boolv) may include [Pachama](https://www.crunchbase.com/organization/pachama), [Noogata](https://www.crunchbase.com/organization/noogata), and [Deepwise](https://www.crunchbase.com/organization/deepwise)
布尔向量BOOLV的竞争对手有哪些？布尔向量 BOOLV 的替代品和可能的竞争对手可能包括 Pachama、Noogata 和 Deepwise





### Logging

20231228 Jack Lee 今天目标推进500词，最终推进到了20492词

今天根据Unite.AI文章， [10 "Best" AI Video Generators (December 2023) - Unite.AI](https://www.unite.ai/best-ai-video-generators/)

[10 个“最佳”AI 视频生成器（2023 年 XNUMX 月） - Unite.AI](https://www.unite.ai/zh-CN/%E6%9C%80%E5%A5%BD%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E5%99%A8/)

主要增加了Pictory

20231227 Jack Lee 今天目标推进500词，最终推进到了19755词

今天主要补充了pika公测的相关消息，还有根据网页 [AI视频工具 | AI工具集导航 ](https://ai-bot.cn/favorites/ai-video-tools/)在AI generated video tool中增加了HeyGen，发现优质信息源Unite.AI

信息源介绍：

Unite.AI：我们精心制作的时事通讯是为那些想要保持行业领先地位的颠覆性企业家、精明的投资者或商业专业人士编写的。 **Unite.AI 的人工智能业务**，具有最新的人工智能突破、新的机器学习方法，以及来自网络各个角落的必知见解。

* [Meet the Team - Unite.AI](https://www.unite.ai/meet-the-team/)

  * Unite.ai was designed to offer detailed analysis and news on the latest advancements in [machine learning](https://www.unite.ai/what-is-machine-learning/) and AI technology. We also want to be a platform to highlight new and upcoming AI companies who are unfortunately not getting the recognition that they deserve from the mainstream media.

    In an effort to help UNITE the AI community, we are also offering access to the top [AI events](https://www.unite.ai/conferences/), [AI courses](https://www.unite.ai/courses/), and [AI newsletters](https://www.unite.ai/ai-newsletters/).

* [认识团队 - Unite.AI](https://www.unite.ai/zh-CN/meet-the-team/)

  * Unite.ai 旨在提供有关最新进展的详细分析和新闻 [机器学习](https://www.unite.ai/zh-CN/什么是机器学习/) 和人工智能技术。 我们还希望成为一个平台，以突出新兴和即将出现的人工智能公司，不幸的是，这些公司没有得到主流媒体应有的认可。

    为了帮助团结 AI 社区，我们还提供接触顶尖人才的机会 [人工智能事件](https://www.unite.ai/zh-CN/会议/), [人工智能课程](https://www.unite.ai/zh-CN/课程/)及 [人工智能通讯](https://www.unite.ai/zh-CN/艾时事通讯/).

![Unite AI](https://gitee.com/junhaoyu/work20221111/raw/master/img/202312281007531.jpg)

20231226 Jack Lee 今天目标推进500词，最终推进到了18725词

昨天整理完了Ben's Bites 的 数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库

今天随机漫游，在google里直接用关键词ai视频生成公司名单数据库搜索，在Foresight News里面发现李飞飞携斯坦福联袂谷歌，推出了用于生成逼真视频的扩散模型 W.A.L.T，另外今天发现我已经获得了pika的资格，不过之前已经获得资格的网友反映pika服务器负载严重导致生成视频很慢，网友卡兹克说pika的Prompt不用太长的，写太复杂的就会直接崩了，Close-up of girl's mobile phone with a heart on the screen, anime style, Ghibli studio，他都用很短的句子。

Foresight News - Foresight Wiki. Foresight News 是亚太地区最具影响力的多语种Web3 媒体，2022 年1 月成立以来，快速成长为最具影响力的中文Web3 媒体之一，目前中文站单一平台月度PV 已突破200 万。

今天主要增加了李飞飞携斯坦福联袂谷歌，推出了用于生成逼真视频的扩散模型 W.A.L.T

20231225 Jack Lee 今天目标推进500词，最终推进到了16826词

今天继续整理 Ben's Bites 的 数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库

今天主要增加了Boolean vector BOOLV公司

20231224 Jack Lee 今天目标推进500词，最终推进到了15796词

今天继续整理 Ben's Bites 的 数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库

今天主要增加了VCAT.AI公司

20231223 Jack Lee 今天目标推进500词，最终推进到了15209词

昨天已经整理完了Ben's Bites 的 数据库1：Airtable - Grid view AI Project Tracker 人工智能项目追踪器的最新数据库

今天整理 Ben's Bites 的 数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库，于是在数据库2的表格里面用关键词**Video generation**进行过滤搜索（Filtered by Company Description 公司描述中含有**Video generation**）

今天主要增加了SeiSei.ai公司

20231222 Jack Lee 今天目标推进500词，最终推进到了14302词

今天主要增加了Imagen Video: A New Text-Conditioned Video Diffusion Model

20231221 Jack Lee 今天目标昨天已完成

20231220 Jack Lee 今天目标推进500词，今天准备的是明天的进度，最终推进到了13698词

今天主要增加了Phenaki: A Model for Generating Videos from Text

20231219 Jack Lee 今天目标推进1000词，因为1221有事情，所以提前准备一天进度，减少明天的压力，最终推进到了12972词

今天主要增加了Elai.io - AI Video Generation Platform、Introducing a Practical Tool to Generate Images and Videos with AI，昨晚从微信公众号看到数字生命卡兹克消息，加入了他的微信群，补充了PIKA即将全面公测的最新消息

20231218 Jack Lee 今天目标推进500词，最终推进到了10443词

20231217 Jack Lee 今天目标推进500词，最终推进到了13685词，然后开始根据阳志平老师之前给的建议，以确保内容的质量不会受到牺牲。保证信息的准确性、分析的深度以及对读者的实际用处为指导，精简一些文字，最终精简到了9745词

今天主要增加了Scalable Adaptive Computation for Iterative Generation和Tags标签的双语格式，参考 OpenMindClub/awesome-chatgpt仓库的首页README.md 统一了本文档中目前存在的维基百科相关词条和论文地址的格式，论文地址部分修改格式为论文提交年份+论文标题，精简了Runway's Gen-2的媒体资讯部分、WonderJourney的描述部分、阿里Animate Anyone、字节跳动的MagicAnimate、微软的GAIA、GitHub - arpitbansal297/Universal-Guided-Diffusion媒体资讯的部分文字

20231216 Jack Lee 今天目标推进500词，最终推进到了13133词

今天继续整理**Ben's Bites**两个数据库中的每天更新的人工智能项目追踪器数据库中的内容

今天主要增加了SceneScape: Text-Driven Consistent Scene Generation

20231215 Jack Lee 今天目标推进500词，最终推进到了12288词

今天继续整理**Ben's Bites**两个数据库中的每天更新的人工智能项目追踪器数据库中的内容

今天主要增加了GitHub - arpitbansal297/Universal-Guided-Diffusion

20231214 Jack Lee 今天目标推进500词，最终推进到了11231词

今天受到首届东木人生发展挑战赛微信群开智校友（川子同学和jack同学）启发，然后主动搜索到了公众号数字生命卡兹克的《盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来》一文。明天继续整理**Ben's Bites**两个每天更新的数据库中的内容

今天主要增加了阿里的Animate Anyone、字节跳动的MagicAnimate**、**微软的GAIA，还补充了一下pika部分的相关内容。

20231213 Jack Lee 今天目标推进500词，最终推进到了4509词

今天主要增加了Gen-1部分和Gen-1 Explained、Gen-2 Explained，Text-to-Video Tool

主要思路是借助OpenMindClub/awesome-chatgpt仓库的首页README.md 中的

- [Find AI Tools Using AI](https://theresanaiforthat.com/?message=subscribed) - AI tools. Updated daily.
  使用 AI 查找 AI 工具 - AI 工具。每日更新。

进一步明确关键词为**Video generation**，配合阳志平老师提供的方法：从自己能拿到的最优质的公开数据，掌握的最优质的信息入手，我由此联想到我拥有**Ben's Bites**两个每天更新的数据库的访问权限，于是在数据库1表格里面用关键词**Video generation**进行过滤搜索（Filtered by Tags 标签中含有**Video generation**），然后完成了今天的500词。

We have two databases that are updated every day;
我们有两个每天更新的数据库；

All 10k+ links we’ve covered, easily filterable
我们涵盖的所有 10k+ 链接都可轻松过滤

6k+ AI company funding rounds from Jan 2022, including investors, amounts, stage etc
2022年1月起超过6k+轮AI公司融资，包括投资者、金额、阶段等

数据库1：Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。可过滤和可搜索。包括相关的筹款和投资者数据。

> It includes all links mentioned in the emails, from the first issue. It'll keep updating over time.
>
> 它包括电子邮件中提到的从第一期开始的所有链接。它将随着时间的推移不断更新。

https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库 https://airtable.com/appLPuy8wEZ8dbiHI/shrNYsG0mStB7NY06/tblBbns8QETdkqrXC

> The database contains of all AI company funding rounds, the amounts, location, investors and much more.
>
> We populate the information where possible but as with private company data, it may be inaccurate or missing, unfortunately. We verify the data from Crunchbase, Pitchbook and other sources where possible.
>
> 该数据库包含所有人工智能公司的融资回合、金额、地点、投资者等信息。
>
> 我们尽可能填充信息，但遗憾的是，与私人公司数据一样，这些信息可能不准确或丢失。我们尽可能从 Crunchbase, Pitchbook 和其他来源核实数据。

20231212  Jack Lee 今天搬运OpenMindClub/awesome-chatgpt仓库的首页README.md&README.zh-cn.md文件并稍作初步修改，然后在List  about the current collection of AI video projects.md中继续增加字数，增加了元资源中维基百科中文本到视频模型词条和生成式人工智能词条，进一步完善了Pika、Gen-2、WonderJourney的介绍 目前字数2771词

PS：本文档中出现的OpenMindClub/awesome-chatgpt仓库统一特指开智学堂的GitHub仓库 [OpenMindClub/awesome-chatgpt: ⚡ Everything about ChatGPT](https://github.com/OpenMindClub/awesome-chatgpt/tree/main#general)

20231211  Jack Lee init 今天推进初稿500词 20000字/42天=477字/天