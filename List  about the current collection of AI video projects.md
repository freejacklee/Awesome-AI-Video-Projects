## 仓库地址：freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects

* [freejacklee/Awesome-AI-Video-Projects: This is an awesome GitHub list of information about the current collection of AI video projects] https://github.com/freejacklee/Awesome-AI-Video-Projects



## 提交内容：List  about the current collection of AI video projects.md

This is an awesome GitHub list of information about the current collection of AI video projects

List  about the current collection of AI video projects



## 元资源

* [Text-to-video model - Wikipedia --- 文本到视频模型 - 维基百科](https://en.wikipedia.org/wiki/Text-to-video_model)

  * A **text-to-video model** is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning) model which takes as input a [natural language](https://en.wikipedia.org/wiki/Natural_language) description and produces a [video](https://en.wikipedia.org/wiki/Video) matching that description.[[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)
    文本到视频模型是一种机器学习模型，它将自然语言描述作为输入并生成与该描述匹配的视频。 [[1\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-AIIR-1)

    Video prediction on making objects realistic in a stable background is performed by using [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) for a sequence to sequence model with a connector [convolutional neural network](https://en.wikipedia.org/wiki/Convolutional_neural_network) encoding and decoding each frame pixel by pixel,[[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) creating video using [deep learning](https://en.wikipedia.org/wiki/Deep_learning).[[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)
    通过使用循环神经网络进行序列到序列模型的视频预测，使对象在稳定的背景下具有连接器卷积神经网络逐像素编码和解码每个帧， [[2\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-2) 使用深度学习创建视频。 [[3\]](https://en.wikipedia.org/wiki/Text-to-video_model#cite_note-3)

* [Generative artificial intelligence - Wikipedia --- 生成人工智能 - 维基百科](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)
  * Video 视频
    * Generative AI trained on annotated video can generate temporally-coherent video clips. Examples include Gen-1 and Gen-2 by [Runway](https://en.wikipedia.org/wiki/Runway_(company))[[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) and Make-A-Video by Meta Platforms.[[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)
      经过带注释视频训练的生成式人工智能可以生成时间连贯的视频剪辑。示例包括 Runway [[44\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-44) 的 Gen-1 和 Gen-2 以及 Meta Platforms 的 Make-A-Video。 [[45\]](https://en.wikipedia.org/wiki/Generative_artificial_intelligence#cite_note-45)



## AI generated video tool

### 1、Pika

Website: https://pika.art 
Discord: http://discord.gg/pika 
About: https://pika.art/about

来自公众号 数字生命卡兹克的用户测评：* [【全网首发】PIKA1.0上手评测 - 你就是传奇](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660666&idx=1&sn=0e9e2a11d5c06cd512479d35ca84bf3f&sharer_shareinfo=d3d03bfa8311534acd00b7003123ed3c&sharer_shareinfo_first=d3d03bfa8311534acd00b7003123ed3c#rd)

我的个人体验：20231210已申请加入waitlist，暂未获得邀请资格，继续期待。我在Pika Labs的Discord上用一张图片生成的视频中人物的脸也会变形。

温馨提示：目前可以在Pika Labs的Discord上直接用文字生成视频

* [Pika Labs在Discord上直接用文字生成视频 #ai #discord #aitools #ai视频 #aivideo #aiviralshorts - YouTube](https://www.youtube.com/watch?v=d_GowRZE2cc)

* [Pika Labs Discord Guide --- Pika Labs Discord 指南](https://pikalabs.org/pika-labs-discord-guide/)



* [AI生成视频工具Pika爆火，估值超2亿美元-虎嗅网](https://m.huxiu.com/article/2361484.html#:~:text=%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%8D%8E%E4%BA%BA%E5%AD%A6%E7%94%9F%E9%80%80%E5%AD%A6%E5%88%9B%E5%8A%9E,%E7%BC%96%E8%BE%91%E5%92%8C%E9%87%8D%E6%96%B0%E6%9E%84%E6%83%B3%E5%9C%BA%E6%99%AF%E3%80%82&text=%F0%9F%92%A5%20Pika%201.0%E4%BD%BF%E7%94%A8AI,%E7%AE%80%E5%8D%95%E4%B8%94%E9%A3%8E%E6%A0%BC%E5%A4%9A%E5%8F%98%E3%80%82)
  * 斯坦福华人学生退学创办的AI视频生成工具Pika 1.0正式推出，估值超过2亿美元。 该工具可以通过文字、图片和视频生成高质量的各种风格视频，并且支持用户上传视频片段进行编辑和重新构想场景。 💥 Pika 1.0使用AI模型生成非常贴近生动的视频，使用简单且风格多变。

* [斯坦福华人博士文生视频Pika 1.0爆火，4人公司估值2亿，OpenAI联创参投-36氪](https://36kr.com/p/2539021165094660) 

  * [新智元](https://36kr.com/user/574825230)*·*发表于2023-11-29 15:30

  * Runway Gen-2最强竞品Pika，暌违半年忽然放出大招——Pika 1.0正式发布！

    仅成立六个月，Pika就结束了测试版，正式发布了第一个产品，能够生成和编辑3D动画、动漫、卡通和电影。


* [斯坦福华人博士文生视频Pika 1.0爆火！4人公司估值2亿，OpenAI联创参投](https://mp.weixin.qq.com/s?__biz=MzU0OTkwNTM2Mw==&mid=2247653802&idx=1&sn=d8597cc271043b01541ff96aa561fc22&sharer_shareinfo=2d57f4d178c2f63c3692057c22223146&sharer_shareinfo_first=2d57f4d178c2f63c3692057c22223146#rd)
  * 在获取Pika 1.0试用资格之前，和Midjourney一样，用户现在通过Discord获取Pika Labs的视频生成服务。用户只需在聊天框输入文字，比如「一个机器人在日落沙滩上行走」，就能收到一个由AI生成的视频。
    周二，Pika把这一体验带到了网页上，面向更广泛的主流群体，让他们可以在编辑视频、自定义物体。这里还有一段，Pika创意总监前几天放出的，用Pika文本转视频AI功能制作的「3D动画预告片」，效果萌到爆。
* [4个人，Pika估值10亿](https://mp.weixin.qq.com/s?__biz=Mzk0ODUwNjUxNQ==&mid=2247484633&idx=1&sn=bf28878224f5ef7205214401a7d5669a&sharer_shareinfo=9978076d64328890c7eb7018395ec70d&sharer_shareinfo_first=9978076d64328890c7eb7018395ec70d#rd)
  * 要了解关于Pika的最新信息，请关注X上的@pika_labs，加入pika的Discord社区并在这里访问我们的测试版产品，并在https://pika.art加入新Pika 1.0的等待名单。

* [中国天才少女硅谷创立AI公司，半年估值超10亿](https://mp.weixin.qq.com/s?__biz=MTI3NTQ1MTY0MQ==&mid=2650603935&idx=1&sn=368bd29cbf96b8095e5541ae6689b229&sharer_shareinfo=eda2394bbef20b16a62fb59bd91e59ac&sharer_shareinfo_first=eda2394bbef20b16a62fb59bd91e59ac#rd)
  * 目前，Pika1.0正式的网页版需要排队预约，尚未有用户实际测评过。有人借此质疑，横空出世的Pika一夜爆红，是否为一场营销骗局？毕竟，在11月之前，Pika还只是一个无名之辈。事实上，Pika的第一个版本今年4月下旬就在Discord上进行了公测。7月，在Discord正式推出服务器，并在几个月时间内收获了50万用户。由于Pika团队精简，寄生在Discord平台，能够最大限度地减少开发量。
    最初，Pika只支持文本生视频，后来逐渐支持图片转视频、相机控制、文字和Logo嵌入视频中等。Pika1.0宣传片中的许多功能，目前Discord上的版本并不支持，只能等网页版开放测评后验证。Pika也并非第一次在众人前亮相。今年11月初，《流浪地球3》的发布会上，电影工业化实验室G!Lab官宣成立。郭帆导演介绍了一批战略合作的科技公司，包括商汤科技、小米、华为等，还有Pika Labs。至今，成立仅6个月的Pika已经完成了三轮融资，总金额5500万美元，估值超10亿元人民币。投资人阵容也可谓豪华——包括OpenAI董事会成员Adam D'Angelo与前特斯拉AI总监Andrej Karpathy、前Github CEO Nat Friedman、YC合伙人Daniel Gross，以及硅谷著名投资人Elad Gil等。

* [Pika (@pika\_labs) / X](https://twitter.com/pika_labs)

* [Pika, which is building AI tools to generate and edit videos, raises $55M | TechCrunch --- Pika 正在构建用于生成和编辑视频的 AI 工具，筹集了 5500 万美元 | TechCrunch](https://techcrunch.com/2023/11/28/pika-labs-which-is-building-ai-tools-to-generate-and-edit-videos-raises-55m/)




@阑夕
AI视频生产，新兴产品Pika和老牌产品Runway
的对比，同一张图片、同样的设置，可以看得出来已经各有千秋了。

来自阑夕的微博视频号 https://weibo.com/tv/show/1034:4977370101383191?from=old_pc_videoshow



### 2、Runway Gen-2

Website: https://research.runwayml.com/gen2

Discord: https://discord.com/invite/runwayml

About: https://research.runwayml.com/about

Date：2023年6月8日

Summary：Runway's Gen-2 allows users to create videos in any style using Text to Video generation. Runway 的 Gen-2 允许用户使用文本到视频生成功能创建任何风格的视频。

Tags：AI, video generation, Runway

Gen-2 Explained：

Not too long ago, runway pushed the boundaries of generative Ai with Gen Onea video to video model that allows you to use words and images to generate new videos out of existing ones in the week since launching, the model has constantly gotten better temporal consistency, better fidelity better results and as more and more people gained access, we unlocked entirely new use cases and displays of creativity and today we're excited to announce our biggest unlock yettext to Video with Gen Two now. You can generate a video with nothing but words, no driving video no input image gen 2 represents yet another major research milestone and another monumental step forward for generative Ai with Gen 2, anyone anywhere can suddenly realize entire worlds, animations stories anything you can imagine gen two coming, very soon to https://runwayml.com/

不久前，runway通过Gen Onea视频到视频模型突破了生成式AI的界限，该模型允许您使用文字和图像从现有视频中生成新视频 自推出以来的一周内，该模型不断获得更好的时间一致性，更好的保真度，更好的结果，并且随着越来越多的人获得访问权限， 我们解锁了全新的用例和创造力展示，今天我们很高兴地宣布，我们迄今为止最大的解锁版本是第二代视频。你可以生成一个只有文字的视频，没有驾驶视频，没有输入图像，第二代代表了另一个重要的研究里程碑，也是生成式人工智能向前迈出的又一重大一步，第二代，任何地方的任何人都可以突然意识到整个世界，动画故事、任何你能想象到的第二代即将到来，很快就会 https://runwayml.com/

个人体验：20231212尝试了一次，用一张弹古筝的女子图片和一句简短的文字生成视频，生成的视频中随着时间流逝人物面部会有点变形。

* [What is Gen-2 AI and How to Use It? A Step-by-Step Guide --- 什么是第二代人工智能以及如何使用它？分步指南](https://ambcrypto.com/blog/what-is-gen-2-and-how-to-use-it-a-step-by-step-guide/)

  * Gen-2 AI is the second generation of Runway’s AI software, which focuses on generating videos from scratch using text descriptions, images, or existing video clips. 
    Gen-2 AI 是 Runway 的第二代 AI 软件，专注于使用文本描述、图像或现有视频剪辑从头开始生成视频。

    This cutting-edge technology opens a realm of possibilities for content creators, allowing them to craft distinctive and captivating videos without resorting to costly equipment or lengthy procedures.
    这项尖端技术为内容创作者打开了一个可能性的领域，使他们能够制作独特且引人入胜的视频，而无需诉诸昂贵的设备或冗长的程序。

    The Gen-2 AI model is designed to help users create dreamy videos by harnessing the power of AI and generative algorithms, allowing for an unparalleled level of customization and fidelity in the final output. 
    Gen-2 AI 模型旨在帮助用户利用 AI 和生成算法的力量来创建梦幻视频，从而在最终输出中实现无与伦比的定制化和保真度。

    With Gen-2 AI, the days of being limited by available footage or budget constraints are numbered, as this innovative technology brings endless creative possibilities to the table.
    有了第二代人工智能，受可用镜头或预算限制的日子已经屈指可数了，因为这项创新技术带来了无限的创意可能性。

* [全面开放，无需排队，Runway视频生成工具Gen-2开启免费试用](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650885185&idx=3&sn=d8042976cd29c23e0c0ba8ee1923a645)
  * Runway 宣布，Gen-1 和 Gen-2 已经彻底开放，任何人都可以注册一个账号免费尝试。生成的视频长度为 4 秒，每秒消耗 5 个积分，利用免费额度可以生成二十几个视频。如果免费积分耗尽，付费标准为 0.01 美元 / 积分，也就是生成一个视频需要 0.2 美元。*2023-07-25 13:24* *机器之心 发表于北京*

* [Gen-2颠覆AI生成视频，一句话秒出4K高清大片，网友：彻底改变游戏规则-36氪](https://36kr.com/p/2501983674115975)

* [只要输入一段话就能生成视频？Gen-2实测\_精彩视频为您呈现\_36氪](https://36kr.com/video/2307964668027272)

* [图像涂哪就动哪，Gen-2新功能“神笔马良”爆火，网友：急急急-36氪](https://36kr.com/p/2515454214115330)

* [视频生成AI卷起来了，一句话一张图就能出大片-虎嗅网](https://m.huxiu.com/article/2054357.html)

* [Runway (@runwayml) / X](https://twitter.com/runwayml)

* [Generative AI’s Next Frontier Is Video - Bloomberg --- 生成式人工智能的下一个前沿领域是视频 - Bloomberg](https://www.bloomberg.com/news/articles/2023-03-20/generative-ai-s-next-frontier-is-video)

* [Runway Gen-2 Update Brings Striking Improvements to AI Video Fidelity and Realism --- Runway Gen-2 更新为 AI 视频保真度和真实度带来了显着改进](https://www.maginative.com/article/runway-gen-2-video-ai-takes-major-step-forward-in-fidelity-and-length/)



#### Runway Gen-1

Website: https://research.runwayml.com/gen1

About: https://research.runwayml.com/about

Paper:  [[2302.03011] Structure and Content-Guided Video Synthesis with Diffusion Models --- [2302.03011]具有扩散模型的结构和内容引导视频合成](https://arxiv.org/abs/2302.03011)

Gen-1 Explained：

Gen 1 is able to realistically and consistently apply the composition and style of an image or text prompt to the target video allowing you to generate new video content using an existing video.We call this approach video to video, and we're incredibly excited to share a few early use casesstylization mode, transfer the style of any image or prompt to every frame of your video storyboard mode, turn mockups into fully stylized and animated rendersmask mode, isolate subjects in your video and modify them with simple text prompts.Render mode, turn untextured renders into realistic outputs by applying an input imageor prompt.
To realizing the future of storytelling.

Gen 1 能够逼真且一致地将图像或文本提示的构图和样式应用于目标视频，从而允许您使用现有视频生成新的视频内容。我们将这种方法称为视频到视频，我们非常高兴地分享一些早期的用例风格化模式，将任何图像或提示的样式传输到视频故事板模式的每一帧，将模型转换为完全风格化和动画渲染蒙版模式，隔离视频中的主题并使用简单的文本提示对其进行修改。渲染模式，通过应用输入图像或提示将无纹理渲染转换为逼真的输出。
实现讲故事的未来。



### 3、WonderJourney

项目及演示：https://kovenyu.com/wonderjourney/
论文：https://arxiv.org/pdf/2312.03884.pdf
GitHub：https://github.com/KovenYu/WonderJourney（Coming soon!）

WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。

它能够根据用户提供的文本描述或图片，**自动生成一系列3D场景的连续画面。**

这些场景不仅多样化，而且**彼此之间还能紧密衔接**，形成一种**虚拟的“奇妙旅程”场景**。

**而且你只需要输入一段描述或上传一张图片即可...**



**主要功能特点：**

与之前专注于单一场景类型的视图生成工作不同，WonderJourney从任何用户提供的位置（通过文本描述或图像）开始，生成一系列多样化但连贯相连的3D场景。

**1、从任意位置出发：**用户可以通过提供一段文本描述或一张图片来指定一个起始点。基于这个起始点WonderJourney将生成一系列3D场景。

例如，如果用户上传一张森林的图片或描述一个城市景观，WonderJourney会从这个场景开始，创造一连串与之相关的3D场景。

**2、长时间的“奇妙之旅”：**WonderJourney能够生成不仅多样化而且持续较长时间的3D场景序列。

用户可以体验一段长时间的虚拟旅程，其中场景会连续不断地变化，提供丰富的视觉体验。

**3、多样化的目的地：**即使从同一个起始点出发，WonderJourney也能生成通往不同“目的地”的多条“奇妙之旅”。

例如，从同一张森林图片出发，一条旅程可能以山脉为终点，而另一条可能以海滩结束，展现出不同的场景和风格。

**4、受控的“奇妙之旅”：**用户可以通过提供一系列文本描述（如诗歌、俳句或故事摘要）来指导生成的旅程。

这允许用户创造更具个性和主题性的旅程。例如，根据一首诗的情感和意象，生成一系列与之相匹配的场景。



**工作原理：**

该框架利用大语言模型（LLM）生成场景的文本描述，一个由文本驱动的点云生成管道来制作引人入胜且连贯的3D场景序列，以及一个视觉语言模型（VLM）来验证生成的场景。

1、场景描述生成：使用大型语言模型（LLM）自动生成场景描述。根据用户输入的文本或图像，LLM提供场景的语义和概念描述。

2、文本驱动的视觉场景生成：根据LLM生成的场景描述，使用文本驱动的视觉场景生成模块创建3D场景。该模块将文本描述转换为彩色点云，形成3D场景。

3、视觉验证：使用视觉语言模型（VLM）对生成的场景进行检查。确保场景没有不希望的视觉效果，如视觉上的错误或不连贯性。

4、连贯性和多样性：生成的3D场景在视觉上连贯，同时在风格和类型上多样化。形成一种连续的视觉旅程，模拟在一个虚拟“奇妙世界”中的体验。

* [小互 on X: "WonderJourney：是一个由斯坦福大学和谷歌合作开发的项目。 它能够根据用户提供的文本描述或图片，自动生成一系列3D场景的连续画面。 这些场景不仅多样化，而且彼此之间还能紧密衔接，形成一种虚拟的“奇妙旅程”场景。 而且你只需要输入一段描述或上传一张图片即可... 主要功能特点：… https://t.co/gptrWSyWBz" / X](https://twitter.com/xiaohuggg/status/1733779657722622449)
  * 发表于2023-12-10 17:24 




* [WonderJourney谷歌合作的 3D 场景生成, 带你走进“奇妙旅程”](https://mp.weixin.qq.com/s?search_click_id=12501261693348582618-1702361382506-3224040202&__biz=Mzg2ODk4MDUxOQ==&mid=2247485556&idx=1&sn=b71b69562cab374580961cd61e05b976#rd)
  * 给定诗歌或故事提要等一系列文本描述,WonderJourney也可以生成古诗词场景。



### 4、Text-to-Video Tool（Create mini AI videos from text）

URL： [TextToVideo | Create videos from text](https://text-to-video.vercel.app/)

Newsletter Post Title：ChatGPT business ideas with a billionaire

Newsletter Post URL：https://bensbites.beehiiv.com/p/government-bans-will-ai-emerge-victorious

Date：2023年4月3日

Summary：This link leads to a website that offers a text-to-video tool. Users can input text and the tool will generate a video based on the content. The website also offers customization options for the video's appearance and background music.

此链接指向一个提供文本转视频工具的网站。用户可以输入文本，该工具将根据内容生成视频。该网站还提供视频外观和背景音乐的自定义选项。

Tags：text-to-video, video generation, AI tool

来自 Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

* [Text to Video AI - Product Information, Latest Updates, and Reviews 2023 | Product Hunt --- 文本转视频 AI - 2023 年产品信息、最新更新和评论 |产品搜索](https://www.producthunt.com/products/text-to-video-ai)

  * Text to Video AI 文本转视频人工智能

  * Launched on March 31st, 2023

    Text to Video is my latest project, which allows you to create videos using AI. Currently AI videos are in their "monstrous stage", just like Dalle 2 MINI a while back. The project seeks that people can have a first approach to text-to-video.
    文本到视频是我的最新项目，它允许您使用人工智能创建视频。目前人工智能视频正处于“怪物阶段”，就像不久前的 Dalle 2 MINI 一样。该项目旨在让人们能够拥有第一种将文本转为视频的方法。



* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * **阿里的Animate Anyone**

    **字节跳动的MagicAnimate**

    **微软的GAIA**

* [阿里、字节悄悄上线AI神器，让梅西跳舞不在话下-36氪](https://36kr.com/p/2549019770755460)

* [阿里大战字节！Animate Anyone vs Magic Animate！AI短视频领域的学术之争！背后技术谁更强？ - YouTube](https://www.youtube.com/watch?v=L-iXbg-GTXk)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下 | 论文频道 | 领研网](https://www.linkresearcher.com/theses/6e61b037-0fbd-4e39-a406-baef6e27afed)

* [全网都在模仿的“科目三”，梅西、钢铁侠、二次元小姐姐马上拿下-36氪](https://36kr.com/p/2543100034213636)

### 5、阿里的Animate Anyone

阿里推出了Animate Anyone，该项目由阿里巴巴智能计算研究院开发，你只需提供一个静态的角色图像（包括真人、动漫/卡通角色等）和一些动作、姿势（比如跳舞、走路），便可将其动画化，同时保留角色的细节特征（如面部表情、服装细节等）。

阿里项目：https://github.com/HumanAIGC/AnimateAnyone

**项目（Animate Anyone官网）地址：**https://humanaigc.github.io/animate-anyone/

阿里论文：https://arxiv.org/pdf/2311.17117.pdf
阿里相关论文发布于2023年11月28日

* [有媒体报道，最近很火... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NwMlp5iDZ?type=repost)

  * 宝玉xp

    23-12-12 13:55
    发布于 美国
    来自 Mac客户端
    有媒体报道，最近很火的阿里巴巴的“Animate Anyone”项目，是通过搜集 TikTok 上网红播主的视频进行训练的

    转译：阿里巴巴的“Animate Anyone”项目，通过搜集著名 TikToker 的视频进行训练

    这个将图像转换为视频的新模型因为人们认为它有可能替代 TikTok 上的网红而在本周迅速走红。然而，这项技术本身就已经内置了从内容创作者那里盗用作品的行为。

    最近，中国零售与科技巨头阿里巴巴的研究团队发表了一篇新论文，介绍了他们的新模型——“Animate Anyone.”。这一消息在网上引发了热议，普遍看法是“TikTokers 的末日来临”，意味着用 AI 技术很快就能取代 TikTok 上的舞蹈内容创作者。

    该模型能够接收输入数据（例如 TikTok 舞蹈视频），并输出新的版本。这次实验的结果相比之前类似尝试略有提升。大多数情况下，他们会复制已有的舞蹈视频，但在服装或风格上有所不同，整体效果略逊一筹。但正如 AI 技术的不断进步，这一模型也将持续优化。

    已有人指出，“Animate Anyone”可能会被滥用，用来制作未经同意的、将人置于虚构场景的视频。实际上，自六年前这项技术问世以来，这已经成为深度伪造技术的主要用途。

    然而，这不仅仅是一个遥远的预测：研究人员已经在未经许可的情况下使用了他人的作品，这已成为他们训练和构建模型的常规做法。阿里巴巴的这篇论文实际上是将最初由明尼苏达大学研究人员出于学术目的创建的“TikTok 数据集”商业化。404 Media 的快速检视显示，阿里巴巴的新 AI 是基于一个抓取了许多知名 TikTok 创作者视频的模型训练而成的，包括 Charli D’Amelio、Addison Rae、Ashley Nocera、Stina Kayy 等几十位。TikTok 数据集中也包括了一些几乎无名的 TikTok 账户用户。

    图二：来自《Animate Anyone》论文的参考图像，动力推动帖，DISCO 模型的示例和阿里巴巴的成果展示。

    在“Animate Anyone”研究论文的网站上，特别展示了一些著名的 TikTok 内容创作者，作为该模型成功运作的例证。在这些案例中，研究人员采用了这些知名 TikTok 影响者的视频作为参考图像，随后通过阿里巴巴开发的模型进行深度处理，制作出较差质量的 AI 生成副本。

    这篇论文及其“Animate Anyone”模型的成果，是建立在未经授权使用创作者作品的基础上的。研究团队在他们的项目页面上，以三位网络名人和艺术家作为示例：Jasmine Chiswell（一位拥有近 1700 万 TikTok 粉丝的生活方式 YouTuber 和 TikTok 名人）、Mackenzie Ziegler（一名歌手和演员，因儿时在《Dance Moms》中出演而知名，拥有 2350 万 TikTok 粉丝）以及 Anna Šulcová（一名 YouTube 内容创作者，拥有 889,600 TikTok 粉丝）。

    这些女性都依靠她们独立的创意工作谋生，但阿里巴巴团队却未经许可地使用了她们的作品来支持他们的研究。论文中还展示了更多的 TikTok 创作者，论文已在 arXiv 预印本服务器上发表。

    阿里巴巴的研究人员在论文中提到，他们使用了包含 340 个训练视频和 100 个测试视频的“TikTok 数据集”，这些视频都是单人舞蹈，时长在 10 到 15 秒之间。该数据集源于 2021 年明尼苏达大学的一个项目，名为“观看社交媒体舞蹈视频来学习穿着人物的高保真深度”，该项目提出了一种用于“估计人体深度和恢复人体形状的方法”，例如使用 AI 技术在视频中为人物更换服装。

    明尼苏达大学的研究人员指出：“我们手动筛选了超过 300 个 TikTok 舞蹈挑战视频，这些视频涵盖了各个月份、不同类型和风格的单人舞蹈。我们选择的舞蹈动作较为温和，以减少运动模糊的产生。对于每个视频，我们都以每秒 30 帧的速度提取了 RGB 图像，总计超过 100,000 张图像。”

    大部分人工智能 (AI) 数据集是由在互联网上，如 TikTok 等社交网络，未经内容所有者同意便擦取的视频、图片和文字构成的。在这个例子中，一些博士生组织并启动的数据集，被全球最大的科技及零售公司之一所使用。

    这样的情况并不罕见：一开始为学术研究而创建的大型数据集，最终被大公司用于商业目的，无论是相似的还是完全不同的。例如，北卡罗来纳大学威尔明顿分校的研究团队就曾从 YouTube 上擦取了跨性别人士上传的视频，并将其整合成一个数据库，用来研发一种能通过面部识别技术识别跨性别人士的技术。

    在当前法律环境日趋严峻的背景下，如阿里巴巴的 AI 研究人员正使用充斥着用户生成内容的擦取数据集。艺术家和其他创作者因 AI 公司未经许可使用他们的作品而提起诉讼。代表艺术家的一起集体诉讼案针对 Midjourney、DeviantArt 和 Stability AI，在去年十月一位法官驳回部分诉求后，又增加了更多原告并提交了修改后的诉状。艺术家们认为，这些 AI 图像生成器复制了原告的作品，并且“创建了与其训练所用作品非常相似的替代品，无论是特定的训练图像还是模仿某些艺术家特有风格的图像，包括原告本人”。

    上个月，一位联邦法官推翻了去年驳回编舞家 Kyle Hanagami 对 Epic Games 诉讼的决定。Hanagami 声称 Fortnite 使用了他的舞蹈动作作为“表情动作”。

    “把编舞简化成‘姿势’，就好比把音乐仅仅看作‘音符’。编舞，本质上是一系列相关连的舞蹈动作和模式，它们被有机地编排成一个完整的作品，”法官如是写道。“这些动作和模式之间的互动，以及编舞者如何创新地将它们融合和安排，构成了这个作品的核心。仅仅是‘姿势’这个元素，远远不能全面展现出一个编舞作品中的创意表达。”

    Hanagami 的律师对 Billboard 表示，推翻之前的驳回裁决将可能对编舞家及其他创意人士在短视频数字媒体时代的权利产生深远影响。这一切对阿里巴巴正在尝试通过“Animate Anyone”项目所打造的内容都有重大的意义，学者们在建立涉及真实人类内容的大型数据集时，也应深思这些决策的未来后果。——来源：* [Alibaba's 'Animate Anyone' Is Trained on Scraped Videos of Famous TikTokers](https://www.404media.co/alibaba-animate-anyone-ai-generated-tiktok/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

* [阿里巴巴智能计算研究院打造Animate Anyone：一种革命性的角色动画技术 | ATYUN.COM 官网-人工智能教程资讯全方位服务平台](https://www.atyun.com/57976.html)

  * 阿里巴巴集团智能计算研究院的研究人员推出了一项先进的角色动画技术“Animate Anyone”，能无缝地把静态图像变成动态的角色视频。该技术利用扩散模型，解决了图像到视频转换中保持时间一致性和细节的难题。

    研究人员在一篇论文中介绍了这项创新的工作。他们设计了一个专为角色动画定制的框架，其中有一个关键的元素，ReferenceNet，能结合参考图像的细节特征，同时保留复杂的外观特征。他们通过空间注意力来实现这一点，保证动画过程中视觉元素的一致性。

    他们还讨论了一个高效的姿态引导器，用来指导角色的动作，保证帧之间的平滑和受控的过渡。他们还采用了一个有效的时间建模方法，保证角色动画中帧间的无缝过渡。

    扩散模型是视觉生成研究的前沿，但从静态图像到视频的转换还有很多挑战，尤其是在保持角色的时间一致性和细节方面。Animate Anyone正是在解决这些问题。如果成功发布，它可能会对Instagram和TikTok上的短视频内容制作者造成威胁。借助参考图像，任何形式的动画，包括360度旋转，都可以用这个框架来实现，实现多功能的视频创作。

    团队承认，他们收到了很多关于演示或在GitHub上获取源代码的询问，他们正在准备公开发布，把一个学术原型变成一个用户友好的原型。但目前还没有具体的日期。

    文章来源：https://analyticsindiamag.com/this-new-ai-tool-could-mark-the-beginning-of-the-end-for-tiktok-and-instagram-influencers/

Animate Anyone在Huggingface上的在线测试地址：暂未发现已经开源 * [AnimateAnyone (AnimateAnyone)](https://huggingface.co/AnimateAnyone)

### 6、字节跳动的MagicAnimate

Magic Animate是一项开创性的开源项目，简化了动画创作，允许您从单个图像和动态[视频](https://www.yjpoo.com/shipinsucai/)制作动画视频，简单来说，给定一张参考图像和一个姿态序列（视频），它可以生成一个跟随姿态运动，并保持参考图像身份特征的动画视频。由新加坡国立大学的Show Lab和字节跳动打造。

Magic Animate在所有舞蹈视频解决方案中提供最高的一致性，但是Magic Animate 面部和手部可能会出现一些扭曲。默认配置可能会导致从动漫到写实主义的风格转变，尤其是在视频中的面部。将动漫风格应用于默认的DensePose驱动视频也会影响身体比例。

**Magic Animate官网地址：**www.magicanimate.org

字节项目：https://github.com/magic-research/magic-animate

字节论文：https://arxiv.org/pdf/2311.16498.pdf

字节相关论文发布于2023年11月27日

MagicAnimate在Huggingface上的在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate

* [抖音跳舞不用真人出镜，一张照片就能生成高质量视频-虎嗅网](https://m.huxiu.com/article/2389861.html?f=rss)

  * 这就是来自新加坡国立大学和字节跳动最新的一项研究，名叫**MagicAnimate**。

    它的作用简单来说可以总结为一个公式：一张**图片** + 一组**动作** = 毫无违和感的**视频**。

* [新加坡国立大学和字节... - @宝玉xp的微博 - 微博](https://weibo.com/1727858283/NvETcaeD1)

  * 宝玉xp

    23-12-5 05:06
    发布于 美国
    来自 微博视频号
    已编辑
    新加坡国立大学和字节跳动联合推出的MagicAnimate，可以用一张照片加上骨骼动画制作小姐姐跳舞视频。这和前几天阿里推出的 AnimateAnyone 网页链接 很像。

    MagicAnimate：通过扩散模型创造时间连贯的人像动画，并提供了 Gradio 演示

    本地演示链接:* [magic-research/magic-animate: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model](https://github.com/magic-research/magic-animate#-gradio-demo?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38)

    本论文深入探讨了人像动画任务——其核心目标是制作一段视频，展示一个特定人物身份按照既定动作序列进行动作。传统的动画制作方法多采用帧变形技术，使参考图片按照目标动作进行动画化。这些方法虽然在一定程度上有效，但在保持动画整体时间连贯性方面存在挑战，主要是由于缺乏对时间序列的建模和对原始人物特征的不够精确保留。在这项研究中，我们提出了 MagicAnimate，这是一个以扩散机制为基础的框架，旨在提高时间连贯性、忠实地保留参考图像以及提升动画的真实感。为了达到这些目标，我们首先开发了一个视频扩散模型，用以捕捉时间信息。接着，为了在不同帧之间保持外观的一致性，我们引入了一个创新的外观编码器，以维持参考图像的精细细节。结合这两大创新，我们还采用了一种简单的视频融合技术，确保长视频动画过程中的平滑过渡。经验证实，我们的方法在两个基准测试中均优于现有的基准方法。特别是在挑战性极高的 TikTok 舞蹈数据集上，我们的方法在视频真实度方面比最强基线提高了超过 38%。我们将公开代码和模型。

    论文：https://arxiv.org/abs/2311.16498
    项目首页：https://showlab.github.io/magicanimate/?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38
    代码：https://github.com/magic-research/magic-animate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38 宝玉xp的微博视频 https://weibo.com/tv/show/1034:4975452565995522?from=old_pc_videoshow
    在线测试地址：https://huggingface.co/spaces/zcxu-eric/magicanimate?continueFlag=4ba8d3bb02db1b4fa268ba0890eadc38

    #微博新知#

阿里、字节两公司在Github上的开源文件还在不断更新中。



### 7、微软的GAIA

项目地址：https://microsoft.github.io/GAIA/       12月14日访问该网页显示404错误

Paper：https://arxiv.org/pdf/2311.15230.pdf

GitHub：https://github.com/microsoft/GAIA  12月14日访问该网页显示404错误

* [How to Create Talking Virtual Characters with Microsoft GAIA - YouTube](https://www.youtube.com/watch?v=mwsfS0dq_bc)

* [Stunning Breakthroughs in AI Creativity! - YouTube](https://www.youtube.com/watch?v=yfxZKoTOka0)

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制 | 机器之心](https://www.jiqizhixin.com/articles/2023-12-04-4)

  * 视频 PS 可以灵活到什么程度？最近，微软的一项研究提供了答案。

    在这项研究中，你只要给 AI 一张照片，它就能生成照片中人物的视频，而且人物的表情、动作都是可以通过文字进行控制的。比如，如果你给的指令是「张嘴」，视频中的人物就会真的张开嘴。

    。。。。。。

    这项研究名叫 GAIA（Generative AI for Avatar，用于虚拟形象的生成式 AI），其 demo 已经开始在社交媒体传播。不少人对其效果表示赞叹，并希望用它来「复活」逝者。

    但也有人担心，这些技术的持续进化会让网络视频变得更加真假难辨，或者被不法分子用于诈骗。看来，反诈手段要继续升级了。

    **GAIA 有什么创新点？**

    会说话的虚拟人物生成旨在根据语音合成自然视频，生成的嘴型、表情和头部姿势应与语音内容一致。以往的研究通过实施特定虚拟人物训练（即为每个虚拟人物训练或调整特定模型），或在推理过程中利用模板视频实现了高质量的结果。最近，人们致力于设计和改进零样本会说话的虚拟人物的生成方法（即仅有一张目标虚拟人物的肖像图片可以用于外貌参考）。不过，这些方法通过采用基于 warping 的运动表示、3D Morphable Model（3DMM）等领域先验来降低任务难度。这些启发式方法虽然有效，但却阻碍了从数据分布中直接学习，并可能导致不自然的结果和有限的多样性。

    本文中，来自微软的研究者提出了 GAIA（Generative AI for Avatar），其能够从语音和单张肖像图片合成自然的会说话的虚拟人物视频，在生成过程中消除了领域先验。

    

    GAIA 揭示了两个关键洞见：

    1. 用语音来驱动虚拟人物运动，而虚拟人物的背景和外貌（appearance）在整个视频中保持不变。受此启发，本文将每一帧的运动和外貌分开，其中外貌在帧之间共享，而运动对每一帧都是唯一的。为了根据语音预测运动，本文将运动序列编码为运动潜在序列，并使用以输入语音为条件的扩散模型来预测潜在序列；
    2. 当一个人在说出给定的内容时，表情和头部姿态存在巨大的多样性，这需要一个大规模和多样化的数据集。因此，该研究收集了一个高质量的能说话的虚拟人物数据集，该数据集由 16K 个不同年龄、性别、皮肤类型和说话风格的独特说话者组成，使生成结果自然且多样化。

* [一张照片生成视频，张嘴、点头、喜怒哀乐，都可以打字控制-36氪](https://36kr.com/p/2543099800561414)

* [盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来](https://mp.weixin.qq.com/s?__biz=MzIyMzA5NjEyMA==&mid=2647660629&idx=1&sn=8b48960cd1cc48496bbb57faff2d4e1f&sharer_shareinfo=650ba074a838b636648c3aefa34d35e6&sharer_shareinfo_first=650ba074a838b636648c3aefa34d35e6#rd)

  * 你可以想象一下，以后再影视、短剧等等的应用场景里面，这种技术会有多离谱的应用。

    当然，还有诈骗。

    这是我目前看到的最牛逼效果最好的照片说话项目。但跟阿里一样，目前弊端就是：非公开。

    但是在项目的主页上也写了：Code (Coming Soon)，也就是代码即将推出。估计很快就能试用了。

* [微软的这个项目厉害了... - @互联网的那点事的微博 - 微博](https://weibo.com/1627825392/Nv8Ya23gw?type=repost)

  * 互联网的那点事

    23-12-1 19:51
    发布于 安徽
    来自 微博视频号
    微软的这个项目厉害了！！

    GAIA的：能够从语音和单张肖像图片合成自然的会说话的头像视频。也就是只需要你的一张照片就能让它开口说话。

    它甚至支持诸如“悲伤”、“张开嘴”或“惊讶”等文本提示，来指导视频生成。

    GAIA还允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

    可以接受语音、视频或文字指令创建会说话的人物头像视频。

    主要功能：

    1、根据语音生成会说话的虚拟人物：如果你给GAIA一个语音录音，它可以创建一个虚拟人物的视频，这个人物的嘴唇和面部表情会跟着语音动。

    2、根据视频生成会说话的虚拟人物：GAIA可以观察一个真人在视频里的动作，然后创建一个虚拟人物模仿这些动作。

    3、控制虚拟人物的头部姿势：你可以告诉GAIA让虚拟人物的头部做出特定的动作，比如点头或摇头。

    项目地址：microsoft.github.io/GAIA/

    4、完全控制虚拟人物的表情：GAIA允许你精确控制虚拟人物的每个面部动作，比如微笑或惊讶的表情。

    5、根据文字指令生成虚拟人物动作：你可以给GAIA一些文字指令，比如“请微笑”，它就会创建一个按照这些指令动作的虚拟人物视频 https://weibo.com/tv/show/1034:4974225992384581?from=old_pc_videoshow

* [Xu Tan on X: "🔥GAIA (Generative AI for Avatar) generates high-quality, natural, and spontaneous avatars given a single reference image driven by text/speech/video. ‼️ Some keywords of GAIA: Data/Model Scaling, ID/Motion Disentanglement, and Zero-Shot. 🔗Project page: https://t.co/qQUXlaAPmU" / X](https://twitter.com/xutan_tx/status/1731007471484027036)
  * GAIA (Generative AI for Avatar)（头像生成式人工智能）通过文本/语音/视频驱动，在单一参考图像的基础上生成高质量、自然、自发的头像。
    GAIA 的一些关键词 数据/模型缩放、ID/运动离散和零镜头。

* [Dreaming Tulpa 🥓👑 on X: "High quality AI generated talking heads are coming! GAIA can generate talking avatars from a single portrait image and speech clip. It even supports text prompts like \`sad\`, \`open mouth\` or \`surprise\` to guide video generation. Crazy times ahead 🤯 https://t.co/20WZOLMypz https://t.co/kgYLyzE1RJ" / X](https://twitter.com/dreamingtulpa/status/1730514359317590234)

  * 高品质人工智能生成的头像来了！

    GAIA 可以从单个肖像图像和语音片段生成会说话的化身。它甚至支持“悲伤”、“张开嘴”或“惊讶”等文字提示来指导视频生成。疯狂的时代即将来临



### 8、GitHub - arpitbansal297/Universal-Guided-Diffusion

论文链接： [[2302.07121v1] Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121v1)

GitHub（代码地址）：https://github.com/arpitbansal297/Universal-Guided-Diffusion

Newsletter Post Title：AI in the workplace

Newsletter Post URL：

* [AI in the workplace](https://bensbites.beehiiv.com/p/ai-workplace)
  * Multimodal universal guidance for diffusion models without retraining.

Date：2023年6月8日

Summary：This is a GitHub repository for Universal Guided Diffusion, which is a machine learning model for image and video generation. 这是通用引导扩散的 GitHub 存储库，通用引导扩散是一种用于图像和视频生成的机器学习模型。

Tags：machine learning, image generation, video generation, GitHub

* [Universal-Guided-Diffusion/README.md at main · arpitbansal297/Universal-Guided-Diffusion](https://github.com/arpitbansal297/Universal-Guided-Diffusion/blob/main/README.md)
  * The official PyTorch implementation of [Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121). This repository has python implementation of universal guidance algorithm that enables controlling diffusion models by arbitrary guidance modalities without the need to retrain any use-specific components. Different guidance modalities we demonstrate are Human Identity, Segmentation Maps, Object Location, Image Style and Clip. Our implementation is based on the text-to-img model from [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and Imagenet Diffusion Model from [OpenAI's guided diffusion](https://github.com/openai/guided-diffusion).
    扩散模型通用指南的官方 PyTorch 实现。该存储库具有通用制导算法的 python 实现，可以通过任意制导方式控制扩散模型，而无需重新训练任何特定用途的组件。我们展示的不同引导模式包括人类身份、分割图、对象位置、图像样式和剪辑。我们的实现基于来自稳定扩散的文本到图像模型和来自 OpenAI 引导扩散的 Imagenet 扩散模型。

* [Universal Guidance for Diffusion Models,arXiv - CS - Machine Learning - X-MOL](https://newsletter.x-mol.com/paper/1626022089827893248?adv)

  * **扩散模型的通用指南**

    典型的扩散模型经过训练可以接受特定形式的调节，最常见的是文本，并且不能在没有重新训练的情况下以其他形式为条件。在这项工作中，我们提出了一种通用的指导算法，使扩散模型能够由任意指导方式控制，而无需重新训练任何特定于使用的组件。我们展示了我们的算法成功地生成了具有引导功能的高质量图像，包括分割、人脸识别、对象检测和分类器信号。代码可在 https://github.com/arpitbansal297/Universal-Guided-Diffusion 获得。

* [扩散模型的通用指导手册\_Zilliz\_InfoQ写作社区](https://xie.infoq.cn/article/c3c7dfb1947a98fddb858a36d)

  * **出品人：Towhee 技术团队 张晨、顾梦佳**

    典型的扩散模型经过训练可以接受特定形式的条件指导（比如文本），但不能在没有重新训练的情况下允许其他形式为条件。 为此研究者提出一种通用的指导算法，使扩散模型无需重新训练任何指定用处的组件，就能由任意指导模式控制。 该算法成功地生成了具有引导功能的高质量图像，包括分割、人脸识别、对象检测和分类器信号。

    论文提出的引导算法增强了扩散模型的图像采样方法，包含了来自现成辅助网络的引导。实验发现，重建的干净图像虽然不够自然和完美，但仍然适用于通用指导函数，以提供具有信息的反馈并指导图像生成。 然后通过拓展分类器指导来激发前向的通用指导，该发现能够被用来处理通用指导。另外，反向通用指导的补充可以帮助强制生成的图像，以满足基于指导函数的约束。该算法最后使用了一种简单但有用的自复现技巧，根据经验提高生成图像的保真度。

    

    

    

    

    



先不管 

Imagen Video

A New Text-Conditioned Video Diffusion Model

一种新的文本条件视频扩散模型



Logging

20231215 Jack Lee 今天目标完成500词，最终推进到了12288词

今天继续整理**Ben's Bites**两个数据库中的每天更新的人工智能项目追踪器数据库中的内容

今天主要增加了GitHub - arpitbansal297/Universal-Guided-Diffusion

20231214 Jack Lee 今天目标完成500词，最终推进到了11231词

今天受到首届东木人生发展挑战赛微信群开智校友（川子同学和jack同学）启发，然后主动搜索到了公众号数字生命卡兹克的《盘一下最近爆火刷屏的3大AI视频项目 - 开始加速的未来》一文。明天继续整理**Ben's Bites**两个每天更新的数据库中的内容

今天主要增加了阿里的Animate Anyone、字节跳动的MagicAnimate**、**微软的GAIA，还补充了一下pika部分的相关内容。

20231213 Jack Lee 今天目标完成500词，最终推进到了4509词

今天主要增加了Gen-1部分和Gen-1 Explained、Gen-2 Explained，Text-to-Video Tool

主要思路是借助https://github.com/OpenMindClub/awesome-chatgpt/tree/main#general 中的

- [Find AI Tools Using AI](https://theresanaiforthat.com/?message=subscribed) - AI tools. Updated daily.
  使用 AI 查找 AI 工具 - AI 工具。每日更新。

进一步明确关键词为**Video generation**，配合阳志平老师提供的方法：从自己能拿到的最优质的公开数据，掌握的最优质的信息入手，我由此联想到我拥有**Ben's Bites**两个每天更新的数据库的访问权限，于是分别在以下两个数据库中用关键词**Video generation**进行搜索，在表格里面用关键词**Video generation**进行过滤搜索（Filtered by Tags），然后完成了今天的500词。

We have two databases that are updated every day;
我们有两个每天更新的数据库；

All 10k+ links we’ve covered, easily filterable
我们涵盖的所有 10k+ 链接都可轻松过滤

6k+ AI company funding rounds from Jan 2022, including investors, amounts, stage etc
2022年1月起超过6k+轮AI公司融资，包括投资者、金额、阶段等

数据库1：Airtable - Grid view AI Project Tracker 人工智能项目追踪器 Ben's Bites 中提到的所有链接的数据库。可过滤和可搜索。包括相关的筹款和投资者数据。

> It includes all links mentioned in the emails, from the first issue. It'll keep updating over time.
>
> 它包括电子邮件中提到的从第一期开始的所有链接。它将随着时间的推移不断更新。

https://airtable.com/appuMJo2TCnijMLkz/shrbLgcCayYdxucC7/tblcTEsr9aeCYdIRw

数据库2：Airtable - Funding Rounds AI Funding Rounds 人工智能融资轮次 人工智能领域所有公司融资轮次的最新数据库 https://airtable.com/appLPuy8wEZ8dbiHI/shrNYsG0mStB7NY06/tblBbns8QETdkqrXC

> The database contains of all AI company funding rounds, the amounts, location, investors and much more.
>
> We populate the information where possible but as with private company data, it may be inaccurate or missing, unfortunately. We verify the data from Crunchbase, Pitchbook and other sources where possible.
>
> 该数据库包含所有人工智能公司的融资回合、金额、地点、投资者等信息。
>
> 我们尽可能填充信息，但遗憾的是，与私人公司数据一样，这些信息可能不准确或丢失。我们尽可能从 Crunchbase, Pitchbook 和其他来源核实数据。

20231212  Jack Lee 今天搬运* [OpenMindClub/awesome-chatgpt: ⚡ Everything about ChatGPT --- OpenMindClub/awesome-chatgpt：⚡ 关于 ChatGPT 的一切](https://github.com/OpenMindClub/awesome-chatgpt/tree/main#general)的README.md&README.zh-cn.md文件并稍作初步修改，然后在List  about the current collection of AI video projects.md中继续增加字数，增加了元资源中维基百科中文本到视频模型词条和生成式人工智能词条，进一步完善了Pika、Gen-2、WonderJourney的介绍 目前字数2771词

20231211  Jack Lee init 今天完成初稿500词 20000字/42天=477字/天